import os
import json
import openai
import re
from tqdm import tqdm
from util import merge_json, parse_json
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Set, Tuple
import hashlib

# í™•ì¥ëœ ë™ì˜ì–´ ì‚¬ì „ (PDF ì œì•ˆì‚¬í•­ ë°˜ì˜)
ENHANCED_SYNONYM_DICT = {
    # íšŒì‚¬ëª… ì •ê·œí™” (ê¸°ì¡´ + í™•ì¥)
    "google": ["êµ¬ê¸€", "Google", "GOOGLE", "êµ¬ê¸€ì½”ë¦¬ì•„", "Google Korea", "êµ¬ê¸€ë¦¬", "êµ¬êµ´"],
    "samsung": ["ì‚¼ì„±", "Samsung", "SAMSUNG", "ì‚¼ì„±ì „ì", "Samsung Electronics", "ì‚¼ì„±ê·¸ë£¹"],
    "naver": ["ë„¤ì´ë²„", "Naver", "NAVER", "ë„¤ì´ë²„í´ë¼ìš°ë“œ", "Naver Cloud", "NHN"],
    "kakao": ["ì¹´ì¹´ì˜¤", "Kakao", "KAKAO", "ì¹´ì¹´ì˜¤í†¡", "KakaoTalk", "ì¹´ì¹´ì˜¤ë±…í¬"],
    "apple": ["ì• í”Œ", "Apple", "APPLE", "ì• í”Œì½”ë¦¬ì•„"],
    "microsoft": ["ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "Microsoft", "MS", "ì— ì—ìŠ¤"],
    "meta": ["ë©”íƒ€", "Meta", "í˜ì´ìŠ¤ë¶", "Facebook"],
    "tesla": ["í…ŒìŠ¬ë¼", "Tesla", "TESLA"],
    "lg": ["LG", "ì—˜ì§€", "ì—˜ì§€ì „ì", "LGì „ì"],
    
    # ìœ„ì¹˜ëª… ì •ê·œí™” (í™•ì¥)
    "ë¯¸êµ­": ["USA", "U.S.", "United States", "ë¯¸í•©ì¤‘êµ­", "ì•„ë©”ë¦¬ì¹´"],
    "ì¤‘êµ­": ["China", "PRC", "ì¤‘í™”ì¸ë¯¼ê³µí™”êµ­"],
    "ì¼ë³¸": ["Japan", "JP", "ì¬íŒ¬"],
    "ì˜êµ­": ["UK", "United Kingdom", "England", "ì‰ê¸€ëœë“œ"],
    "í•œêµ­": ["Korea", "South Korea", "ëŒ€í•œë¯¼êµ­", "ROK"],
    
    # ê¸°ìˆ  ìš©ì–´
    "ai": ["AI", "ì¸ê³µì§€ëŠ¥", "Artificial Intelligence"],
    "iot": ["IoT", "ì‚¬ë¬¼ì¸í„°ë„·", "Internet of Things"]
}

# ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
REVERSE_SYNONYM = {}
for canonical, synonyms in ENHANCED_SYNONYM_DICT.items():
    for syn in synonyms:
        REVERSE_SYNONYM[syn.lower()] = canonical

# í•œêµ­ì–´ ì´ë¦„ íŒ¨í„´ (ê°œì„ ëœ ì •ê·œì‹)
KOREAN_NAME_PATTERNS = [
    r'[ê°€-í£]{2,4}(?:\s+[ê°€-í£]{1,2})*',  # ì¼ë°˜ í•œêµ­ì–´ ì´ë¦„
    r'[ê°€-í£]+\s*(?:íšŒì¥|ì‚¬ì¥|ëŒ€í‘œ|ë¶€ì‚¬ì¥|ì´ì‚¬|íŒ€ì¥|ê³¼ì¥|ë¶€ì¥|ì‹¤ì¥|ì„¼í„°ì¥)',  # ì§ì±… í¬í•¨
]

def enhanced_normalize_entity_name(name: str, entity_type: str, context: str = "") -> str:
    """í–¥ìƒëœ ì—”í‹°í‹° ì´ë¦„ ì •ê·œí™” (PDF ì œì•ˆì‚¬í•­ ë°˜ì˜)"""
    name = name.strip()
    
    # íšŒì‚¬/ì¡°ì§ëª…ì˜ ê²½ìš° ë™ì˜ì–´ ì²˜ë¦¬
    if entity_type in ["COMPANY", "ORGANIZATION"]:
        normalized = REVERSE_SYNONYM.get(name.lower())
        if normalized:
            return normalized.capitalize()
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ê·œí™” (ì˜ˆ: "ì‚¼ì„±"ì´ ì „ìì œí’ˆ ë§¥ë½ì—ì„œ ë‚˜ì˜¤ë©´ "ì‚¼ì„±ì „ì"ë¡œ)
        if "ì‚¼ì„±" in name and any(tech in context for tech in ["ì „ì", "ìŠ¤ë§ˆíŠ¸í°", "ë°˜ë„ì²´", "ë””ìŠ¤í”Œë ˆì´"]):
            return "ì‚¼ì„±ì „ì"
    
    # ì¸ë¬¼ëª…ì˜ ê²½ìš° ê³µë°± ì •ê·œí™” ë° ì§ì±… ë¶„ë¦¬
    if entity_type == "PERSON":
        # í•œê¸€ ì´ë¦„ì˜ ê²½ìš° ë„ì–´ì“°ê¸° ì œê±°
        if re.match(r'^[ê°€-í£\s]+$', name):
            # ì§ì±… ë¶„ë¦¬
            for pattern in ["íšŒì¥", "ì‚¬ì¥", "ëŒ€í‘œ", "ë¶€ì‚¬ì¥", "ì´ì‚¬", "íŒ€ì¥", "ê³¼ì¥", "ë¶€ì¥", "ì‹¤ì¥"]:
                if pattern in name:
                    clean_name = name.replace(pattern, "").strip()
                    return clean_name
            return name.replace(' ', '')
    
    # ìœ„ì¹˜ëª…ì˜ ê²½ìš° í‘œì¤€í™”
    if entity_type in ["LOCATION", "COUNTRY"]:
        normalized = REVERSE_SYNONYM.get(name.lower())
        if normalized:
            return normalized
    
    return name

def extract_with_ner_fallback(text: str, existing_entities: Set[str]) -> List[Dict]:
    """NER ê¸°ë°˜ í´ë°± ì—”í‹°í‹° ì¶”ì¶œ (PDF ì œì•ˆì‚¬í•­)"""
    additional_entities = []
    
    # í•œêµ­ì–´ ì´ë¦„ íŒ¨í„´ ë§¤ì¹­
    for pattern in KOREAN_NAME_PATTERNS:
        matches = re.finditer(pattern, text)
        for match in matches:
            candidate = match.group().strip()
            # ê¸°ì¡´ ì—”í‹°í‹°ì™€ ì¤‘ë³µ í™•ì¸
            if candidate not in existing_entities and len(candidate) >= 2:
                # ì§ì±…ì´ í¬í•¨ëœ ê²½ìš° PERSONìœ¼ë¡œ ë¶„ë¥˜
                if any(title in candidate for title in ["íšŒì¥", "ì‚¬ì¥", "ëŒ€í‘œ", "ë¶€ì‚¬ì¥", "ì´ì‚¬"]):
                    role = None
                    for title in ["íšŒì¥", "ì‚¬ì¥", "ëŒ€í‘œ", "ë¶€ì‚¬ì¥", "ì´ì‚¬"]:
                        if title in candidate:
                            role = title
                            break
                    clean_name = candidate.replace(role or "", "").strip()
                    additional_entities.append({
                        "label": "PERSON",
                        "name": clean_name,
                        "properties": {"role": role} if role else {}
                    })
    
    # ëŒ€ë¬¸ì ì˜ì–´ ë‹¨ì–´ (íšŒì‚¬ëª… í›„ë³´)
    english_company_pattern = r'\b[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+)*\b'
    matches = re.finditer(english_company_pattern, text)
    for match in matches:
        candidate = match.group().strip()
        if candidate not in existing_entities and len(candidate) >= 3:
            # ì¼ë°˜ì ì¸ ì˜ì–´ ë‹¨ì–´ëŠ” ì œì™¸
            common_words = {'THE', 'AND', 'OR', 'BUT', 'FOR', 'WITH', 'THIS', 'THAT'}
            if candidate.upper() not in common_words:
                additional_entities.append({
                    "label": "COMPANY",
                    "name": candidate,
                    "properties": {}
                })
    
    return additional_entities

class EnhancedEntityTracker:
    """í–¥ìƒëœ ì—”í‹°í‹° ì¶”ì ê¸° (ë³„ì¹­ ì§€ì›)"""
    def __init__(self):
        self.entities = {}  # fingerprint -> entity
        self.name_to_fingerprints = {}  # name -> set of fingerprints
        self.aliases = {}  # fingerprint -> set of aliases
        
    def add_entity(self, entity: Dict, context: str = "") -> bool:
        """ì—”í‹°í‹° ì¶”ê°€ (ë³„ì¹­ ì •ë³´ í¬í•¨)"""
        original_name = entity['name']
        fingerprint = self.generate_entity_fingerprint(entity, context)
        
        if fingerprint in self.entities:
            # ê¸°ì¡´ ì—”í‹°í‹°ì˜ ì†ì„± ì—…ë°ì´íŠ¸ ë° ë³„ì¹­ ì¶”ê°€
            existing = self.entities[fingerprint]
            for key, value in entity.get('properties', {}).items():
                if key not in existing.get('properties', {}):
                    existing['properties'][key] = value
            
            # ë³„ì¹­ ì¶”ê°€
            if fingerprint not in self.aliases:
                self.aliases[fingerprint] = set()
            self.aliases[fingerprint].add(original_name)
            return False
        
        # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        entity['name'] = enhanced_normalize_entity_name(entity['name'], entity['label'], context)
        self.entities[fingerprint] = entity
        
        # ë³„ì¹­ ì´ˆê¸°í™”
        if fingerprint not in self.aliases:
            self.aliases[fingerprint] = set()
        self.aliases[fingerprint].add(original_name)
        if original_name != entity['name']:
            self.aliases[fingerprint].add(entity['name'])
        
        # ì´ë¦„ë³„ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        name_lower = entity['name'].lower()
        if name_lower not in self.name_to_fingerprints:
            self.name_to_fingerprints[name_lower] = set()
        self.name_to_fingerprints[name_lower].add(fingerprint)
        
        return True
    
    def generate_entity_fingerprint(self, entity: Dict, context: str = "") -> str:
        """ì—”í‹°í‹°ì˜ ê³ ìœ  ì§€ë¬¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤)"""
        normalized_name = enhanced_normalize_entity_name(entity['name'], entity['label'], context)
        fingerprint = f"{entity['label']}:{normalized_name}".lower()
        return hashlib.md5(fingerprint.encode()).hexdigest()
    
    def get_all_entities(self) -> List[Dict]:
        """ëª¨ë“  ì—”í‹°í‹°ë¥¼ ë³„ì¹­ ì •ë³´ì™€ í•¨ê»˜ ë°˜í™˜"""
        result = []
        for fingerprint, entity in self.entities.items():
            if fingerprint in self.aliases:
                entity['properties']['aliases'] = list(self.aliases[fingerprint])
            result.append(entity)
        return result

# ì „ì—­ ì¶”ì ê¸°
enhanced_entity_tracker = EnhancedEntityTracker()

def enhanced_process_file(idx, filename, chunks_dir, result_dir, schema_json, api_key, system_msg, purpose, existing_names):
    """í–¥ìƒëœ íŒŒì¼ë³„ ë…¸ë“œ ì¶”ì¶œ (Few-shot ì˜ˆì‹œ í¬í•¨)"""
    client = openai.OpenAI(api_key=api_key)
    filename_path = os.path.join(chunks_dir, filename)
    
    if not os.path.exists(filename_path):
        print(f"íŒŒì¼ ì—†ìŒ: {filename_path} â†’ ê±´ë„ˆëœ€")
        return None
    
    with open(filename_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # Few-shot ì˜ˆì‹œ ì¶”ê°€ (PDF ì œì•ˆì‚¬í•­)
    few_shot_example = """
### ì˜ˆì‹œ:
ì…ë ¥ í…ìŠ¤íŠ¸: "ì‚¼ì„±ì „ìì˜ ì´ì¬ìš© íšŒì¥ì´ ë¯¸êµ­ ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì— ìˆëŠ” êµ¬ê¸€ ë³¸ì‚¬ë¥¼ ë°©ë¬¸í•˜ì—¬ AI ê¸°ìˆ  í˜‘ë ¥ì— ëŒ€í•´ ë…¼ì˜í–ˆë‹¤."
ì¶œë ¥:
{
  "nodes": [
    {"label": "PERSON", "name": "ì´ì¬ìš©", "properties": {"role": "íšŒì¥"}},
    {"label": "COMPANY", "name": "ì‚¼ì„±ì „ì", "properties": {"industry": "ì „ì"}},
    {"label": "LOCATION", "name": "ì‹¤ë¦¬ì½˜ë°¸ë¦¬", "properties": {"country": "ë¯¸êµ­"}},
    {"label": "COMPANY", "name": "êµ¬ê¸€", "properties": {"industry": "ê¸°ìˆ "}},
    {"label": "CONCEPT", "name": "AIê¸°ìˆ í˜‘ë ¥", "properties": {"type": "ê¸°ìˆ í˜‘ë ¥"}}
  ],
  "relations": [
    {"start_node": "ì´ì¬ìš©", "relationship": "WORKS_FOR", "end_node": "ì‚¼ì„±ì „ì", "properties": {"position": "íšŒì¥"}},
    {"start_node": "êµ¬ê¸€", "relationship": "LOCATED_IN", "end_node": "ì‹¤ë¦¬ì½˜ë°¸ë¦¬", "properties": {}},
    {"start_node": "ì´ì¬ìš©", "relationship": "VISITED", "end_node": "êµ¬ê¸€", "properties": {"purpose": "AIê¸°ìˆ í˜‘ë ¥ë…¼ì˜"}}
  ]
}
"""
    
    existing_names_str = ""
    if existing_names:
        existing_names_str = f"""
### ì´ë¯¸ ì¶”ì¶œëœ ì—”í‹°í‹° (ì¼ê´€ì„± ìœ ì§€):
{', '.join(sorted(existing_names)[:50])}
"""
    
    prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ **'{purpose}'** ëª©ì ì˜ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ ë…¸ë“œì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

### ìŠ¤í‚¤ë§ˆ:
{json.dumps(schema_json, ensure_ascii=False, indent=2)}

{few_shot_example}

### í•µì‹¬ ì§€ì¹¨:
1. **ì—”í‹°í‹° í†µí•©**: ë™ì¼ ê°œì²´ì˜ ë‹¤ë¥¸ í‘œí˜„ì„ ì¸ì‹í•˜ê³  í†µì¼
   - "êµ¬ê¸€"ê³¼ "Google" â†’ "êµ¬ê¸€"ë¡œ í†µì¼
   - "ì‚¼ì„±"ê³¼ "ì‚¼ì„±ì „ì" â†’ ë¬¸ë§¥ìƒ ê°™ìœ¼ë©´ "ì‚¼ì„±ì „ì"

2. **ì™„ì „í•œ ì¶”ì¶œ**: ëª¨ë“  ì¤‘ìš”í•œ ì—”í‹°í‹° í¬ì°©
   - ëŒ€ëª…ì‚¬ ì°¸ì¡° í•´ê²° ("ê·¸", "ê·¸ë…€", "ì´ íšŒì‚¬" ë“±)
   - ë¬¸ë§¥ìƒ ì–¸ê¸‰ëœ ëª¨ë“  ì¸ë¬¼, íšŒì‚¬, ì¥ì†Œ, ì´ë²¤íŠ¸

3. **ê´€ê³„ ì¶”ì¶œ**: ëª…ì‹œì /ì•”ì‹œì  ê´€ê³„ ëª¨ë‘ í¬í•¨
   - ì§ì ‘ì  ê´€ê³„: "Aê°€ Bì—ì„œ ì¼í•œë‹¤"
   - ì•”ì‹œì  ê´€ê³„: "A íšŒì¥" â†’ A WORKS_FOR [íšŒì‚¬]

4. **ì†ì„± í’ë¶€í™”**: ê°€ëŠ¥í•œ í•œ ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í¬í•¨
   - ì§ì±…, ìœ„ì¹˜, ë‚ ì§œ, ì—­í•  ë“±

{existing_names_str}

### í…ìŠ¤íŠ¸:
{content}

### ì¶œë ¥ (JSON):"""

    print(f"[{idx}] {filename} ì²˜ë¦¬ ì¤‘...")
    
    response = client.chat.completions.create(
        model="gpt-4o",  # ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš© (PDF ì œì•ˆ)
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1  # ë” ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì¶¤
    )
    
    gpt_output = response.choices[0].message.content
    parsed_json = parse_json(gpt_output)
    
    # LLM ì¶”ì¶œ ê²°ê³¼
    llm_entities = set()
    normalized_nodes = []
    for node in parsed_json.get('nodes', []):
        node['name'] = enhanced_normalize_entity_name(node['name'], node['label'], content)
        if enhanced_entity_tracker.add_entity(node, content):
            normalized_nodes.append(node)
        llm_entities.add(node['name'])
    
    # NER í´ë°±ìœ¼ë¡œ ì¶”ê°€ ì—”í‹°í‹° ì¶”ì¶œ (PDF ì œì•ˆì‚¬í•­)
    additional_entities = extract_with_ner_fallback(content, llm_entities)
    for entity in additional_entities:
        entity['name'] = enhanced_normalize_entity_name(entity['name'], entity['label'], content)
        if enhanced_entity_tracker.add_entity(entity, content):
            normalized_nodes.append(entity)
    
    # ê´€ê³„ ì •ê·œí™”
    normalized_relations = []
    for rel in parsed_json.get('relations', []):
        # ê´€ê³„ íƒ€ì… í‘œì¤€í™”
        rel_type = rel['relationship']
        if rel_type in ['EMPLOYED_BY', 'WORKS_AT']:
            rel_type = 'WORKS_FOR'
        elif rel_type in ['HEADQUARTERED_IN', 'BASED_IN']:
            rel_type = 'LOCATED_IN'
        
        rel['relationship'] = rel_type
        normalized_relations.append(rel)
    
    result = {
        'nodes': normalized_nodes,
        'relations': normalized_relations
    }
    
    # ê°œë³„ ê²°ê³¼ ì €ì¥
    result_path = os.path.join(result_dir, f'result_{idx}.json')
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=4)
    
    print(f"[{idx}] ë…¸ë“œ ì¶”ì¶œ ì™„ë£Œ - ì‹ ê·œ ë…¸ë“œ: {len(normalized_nodes)}ê°œ (LLM: {len(parsed_json.get('nodes', []))}, NER: {len(additional_entities)})")
    return result_path, result

def main(purpose="ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„"):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
    result_dir = os.path.join(OUTPUT_ROOT, "result")
    chunks_dir = os.path.join(OUTPUT_ROOT, "chunked_document")
    schema_dir = os.path.join(OUTPUT_ROOT, "schema")
    os.makedirs(result_dir, exist_ok=True)
    
    # ìŠ¤í‚¤ë§ˆ ë¡œë“œ
    with open(f"{schema_dir}/schema.json", "r", encoding="utf-8") as f:
        schema_json = json.load(f)
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    
    system_msg = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì •í™•í•˜ê³  ì™„ì „í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë™ì¼í•œ ê°œì²´ì˜ ë‹¤ë¥¸ í‘œí˜„ì„ ì¸ì‹í•˜ê³  ì¼ê´€ëœ ì´ë¦„ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
ì•”ì‹œì  ê´€ê³„ì™€ ëŒ€ëª…ì‚¬ ì°¸ì¡°ë„ í•´ê²°í•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""
    
    file_list = sorted(os.listdir(chunks_dir))
    n_files = len(file_list)
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì¼ê´€ì„± í–¥ìƒ
    chunk_size = 3  # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¼ê´€ì„± í™•ë³´
    pbar = tqdm(range(0, n_files, chunk_size), desc="Processing batches", unit="batch")
    
    for start_idx in pbar:
        batch_files = file_list[start_idx:start_idx+chunk_size]
        existing_names = {entity['name'] for entity in enhanced_entity_tracker.get_all_entities()}
        
        with ThreadPoolExecutor(max_workers=chunk_size) as executor:
            futures = []
            for i, filename in enumerate(batch_files):
                idx = start_idx + i
                future = executor.submit(
                    enhanced_process_file, idx, filename, chunks_dir, result_dir, 
                    schema_json, api_key, system_msg, purpose, existing_names
                )
                futures.append(future)
            
            for future in as_completed(futures):
                future.result()
    
    # ìµœì¢… ê²°ê³¼ ìƒì„± (ë³„ì¹­ ì •ë³´ í¬í•¨)
    final_result = {
        'nodes': enhanced_entity_tracker.get_all_entities(),
        'relations': []
    }
    
    # ëª¨ë“  ê´€ê³„ ìˆ˜ì§‘
    for idx in range(n_files):
        path = os.path.join(result_dir, f"result_{idx}.json")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                final_result['relations'].extend(data.get('relations', []))
    
    # ìµœì¢… ì €ì¥
    result_path = os.path.join(result_dir, "result.json")
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=4)
    
    print(f"âœ… í–¥ìƒëœ ë…¸ë“œ ì¶”ì¶œ ì™„ë£Œ: {result_path}")
    print(f"ğŸ“Š ì´ ë…¸ë“œ: {len(final_result['nodes'])}ê°œ")
    print(f"ğŸ“Š ì´ ê´€ê³„: {len(final_result['relations'])}ê°œ")
    print(f"ğŸ“Š ë³„ì¹­ ì •ë³´ê°€ í¬í•¨ëœ ë…¸ë“œ: {sum(1 for n in final_result['nodes'] if 'aliases' in n.get('properties', {}))}")

if __name__ == "__main__":
    main(purpose="ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„")