# filename: crawling.py
"""ë„¤ì´ë²„ **ì—°ì˜ˆâ†’ë“œë¼ë§ˆ** ì„¹ì…˜(ëª¨ë°”ì¼) ê¸°ì‚¬ Nê°œë¥¼ í¬ë¡¤ë§í•´ ì„ë² ë”©Â·FAISS ì¸ë±ìŠ¤ë¥¼ ë§Œë“  ë’¤
   í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥/ì €ì¥í•œë‹¤. ëª¨ë“  ì‚°ì¶œë¬¼ì€ ./output/news í´ë”ì— ì €ì¥ëœë‹¤.
   â”€ ì—…ë°ì´íŠ¸: 2025â€‘05â€‘12 (ì™„ì „íŒ: mid=shm, main() í¬í•¨)
"""

from __future__ import annotations

import os
import json
import time
import itertools
from typing import List, Tuple

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv

# ---------------------- 0. í™˜ê²½ ë° ê²½ë¡œ ì„¤ì • ----------------------
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY:
    raise RuntimeError("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”!")

client = OpenAI(api_key=API_KEY)

# ì‚°ì¶œë¬¼ì„ ì €ì¥í•  í´ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")   # ê¸°ë³¸ê°’ "output"
OUTPUT_DIR = os.path.join(OUTPUT_ROOT, "news")
os.makedirs(OUTPUT_DIR, exist_ok=True)

TOP_N     = int(os.getenv("TOP_N", "100"))        # í¬ë¡¤ë§Â·ì„ë² ë”©í•  ê¸°ì‚¬ ìˆ˜
EMB_MODEL = "text-embedding-3-small"               # 256/512ì°¨ì›

# íŒŒì¼ ê²½ë¡œ ---------------------------------------------------------
JSON_PATH   = f"{OUTPUT_DIR}/naver_news_top{TOP_N}.json"
IDX_PATH    = f"{OUTPUT_DIR}/news_index_{TOP_N}.faiss"
VEC_PATH    = f"{OUTPUT_DIR}/news_vectors_{TOP_N}.npy"
RESULT_PATH = f"{OUTPUT_DIR}/keyword_results.json"

# ---------------------- 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ----------------------
MOBILE_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) "
        "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
    ),
    "Referer": "https://m.entertain.naver.com/",
}

SECTION_URL = "https://news.naver.com/main/list.naver"
PARAMS_BASE_DEFAULT = {
    "mode": "LS2D",
    "mid": "sec",
    "sid1": "101",  # ê²½ì œ
    "sid2": "771",  # ê¸°ì—…Â·ê²½ì˜
}
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://www.naver.com",
}

def reset_paths(output_root: str):
    if not output_root:
        output_root = "output"
    global OUTPUT_ROOT, OUTPUT_DIR, JSON_PATH, IDX_PATH, VEC_PATH, RESULT_PATH
    OUTPUT_ROOT = output_root
    OUTPUT_DIR  = os.path.join(OUTPUT_ROOT, "news")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    JSON_PATH   = f"{OUTPUT_DIR}/naver_news_top{TOP_N}.json"
    IDX_PATH    = f"{OUTPUT_DIR}/news_index_{TOP_N}.faiss"
    VEC_PATH    = f"{OUTPUT_DIR}/news_vectors_{TOP_N}.npy"
    RESULT_PATH = f"{OUTPUT_DIR}/keyword_results.json"

def crawl_top_n(n: int = TOP_N) -> List[dict]:
    """ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ ê¸°ì—…Â·ê²½ì˜ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ nê°œ ìˆ˜ì§‘"""
    print(f"ğŸ“°  ë„¤ì´ë²„ ê¸°ì‚¬ {n}ê°œ í¬ë¡¤ë§ ì¤‘â€¦")
    links, page = [], 1
    today = datetime.now().strftime("%Y%m%d")

    # â‘  ë§í¬ ìˆ˜ì§‘
    while len(links) < n:
        params = PARAMS_BASE | {"page": str(page), "date": today}
        res = requests.get(SECTION_URL, headers=HEADERS, params=params, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        new_links = [
            ("https://news.naver.com" + a["href"] if a["href"].startswith("/") else a["href"])
            for a in soup.select("div.list_body.newsflash_body ul li dl dt a")
            if (txt := a.get_text(strip=True)) and txt != "ë™ì˜ìƒê¸°ì‚¬"
        ]
        if not new_links:
            break  # ë” ì´ìƒ ê¸°ì‚¬ ì—†ìŒ
        for l in new_links:
            if l not in links:
                links.append(l)
                if len(links) >= n:
                    break
        page += 1
        time.sleep(0.1)

    # â‘¡ ë³¸ë¬¸ íŒŒì‹±
    def parse(url: str) -> dict:
        r = requests.get(url, headers=HEADERS, timeout=10)
        r.raise_for_status()
        s = BeautifulSoup(r.text, "html.parser")

    title_tag = (
        s.select_one("h2.media_end_head_headline")
        or s.select_one("h2.media_end_head_title")
        or s.select_one("#articleTitle")
        or s.select_one("h3.end_tit")
    )
    title = title_tag.get_text(strip=True) if title_tag else ""

    body_tag = (
        s.select_one("#newsct_article")
        or s.select_one("#articleBodyContents")
        or s.select_one("#newsEndContents")
    )
    content = ""
    if body_tag:
        for dirty in body_tag.select(
            "script, style, .end_photo_org, .link_news, span.media_end_linked_more_url"
        ):
            dirty.decompose()
        content = body_tag.get_text("\n", strip=True)
    return {"title": title, "content": content}

# --------------------------------------------------------------
# 1) ê¸°ì‚¬ í¬ë¡¤ëŸ¬
# --------------------------------------------------------------

def crawl_top_n(n: int = TOP_N, params_base: dict | None = None) -> List[dict]:
    """ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ ì—°ì˜ˆâ†’ë“œë¼ë§ˆ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ nê°œ ìˆ˜ì§‘"""
    params_base = params_base or PARAMS_BASE_DEFAULT
    print(f"ğŸ“°  ë„¤ì´ë²„ ë“œë¼ë§ˆ ê¸°ì‚¬ {n}ê°œ í¬ë¡¤ë§ ì¤‘â€¦")

    links: list[str] = []
    for page in itertools.count(1):
        soup = BeautifulSoup(_fetch_list(params_base, page), "html.parser")

        boxes = (
            soup.select("ul.type06_headline li dl dt:not(.photo) > a")
            or soup.select("ul.type06 li dl dt:not(.photo) > a")
        )
        if not boxes:
            break

        for a in boxes:
            href = a["href"]
            link = "https://news.naver.com" + href if href.startswith("/") else href
            if link not in links:
                links.append(link)
                if len(links) >= n:
                    break
        if len(links) >= n:
            break
        time.sleep(0.1)

    # -------- ê¸°ì‚¬ ë³¸ë¬¸ íŒŒì‹± --------
    articles: list[dict] = []
    for i, url in enumerate(links, 1):
        try:
            art = _parse_article(url)
            articles.append(art)
            print(f"[{i:02d}/{n}] {art['title'][:60]}â€¦")
        except Exception as e:
            print(f"  â†’ ì‹¤íŒ¨({e})")

    with open(JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ…  í¬ë¡¤ë§ ì™„ë£Œ â†’ {JSON_PATH} ì €ì¥\n")
    return articles

# --------------------------------------------------------------
# 2) ì„ë² ë”© & ì¸ë±ìŠ¤ ë¹Œë“œ
# --------------------------------------------------------------

def _get_embedding(text: str) -> List[float]:
    text = text.replace("\n", " ")
    while True:
        try:
            return client.embeddings.create(input=[text], model=EMB_MODEL).data[0].embedding
        except Exception as e:
            print("  â³ ì¬ì‹œë„:", e)
            time.sleep(2)


def build_or_load_index() -> Tuple[faiss.IndexFlatIP, List[dict]]:
    """ê¸°ì‚¬ JSON â†’ ì„ë² ë”© â†’ FAISS ì¸ë±ìŠ¤. ìºì‹œ í™œìš©"""
    if not os.path.exists(JSON_PATH):  # JSON ì—†ìœ¼ë©´ í¬ë¡¤ë§
        crawl_top_n(TOP_N)

    with open(JSON_PATH, encoding="utf-8") as f:
        articles: List[dict] = json.load(f)

    if len(articles) < TOP_N:  # ê¸°ì‚¬ ë¶€ì¡± ì‹œ ì¬í¬ë¡¤ë§
        articles = crawl_top_n(TOP_N)

    # ìºì‹œëœ ë²¡í„°Â·ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(IDX_PATH) and os.path.exists(VEC_PATH):
        xb = np.load(VEC_PATH).astype("float32")
        dim = xb.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(xb)
        return index, articles

    # ìƒˆë¡œ ì„ë² ë”© ìƒì„±
    print("\nğŸ§®  ì„ë² ë”© ìƒì„± ì¤‘â€¦")
    vectors = []
    for art in tqdm(articles, desc="ì„ë² ë”©"):
        txt = (art["title"] + "\n\n" + art["content"])[:3500]
        vec = np.asarray(get_embedding(txt), dtype=np.float32)
        vec /= np.linalg.norm(vec) + 1e-10
        vectors.append(vec)

    xb = np.vstack(vectors).astype("float32")
    np.save(VEC_PATH, xb)

    dim = xb.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(xb)
    faiss.write_index(index, IDX_PATH)
    print("âœ…  ì„ë² ë”© & ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ\n")
    return index, articles

# ---------------------- 3. í‚¤ì›Œë“œ ê²€ìƒ‰ ----------------------

def search_by_keywords(keywords: List[str], top_k: int = 50) -> List[dict]:
    query = " ".join(keywords)
    q_vec = np.asarray(get_embedding(query), dtype=np.float32)
    q_vec /= np.linalg.norm(q_vec) + 1e-10

    index, articles = build_or_load_index()
    D, I = index.search(q_vec[None, :], top_k)

    results = []
    for score, idx in zip(D[0], I[0]):
        art = articles[int(idx)].copy()
        art["score"] = float(score)
        results.append(art)
    return results

# ---------------------- 4. ì‹¤í–‰ ----------------------
def main(params_base):
    global PARAMS_BASE
    PARAMS_BASE = params_base
    output_root = os.getenv("OUTPUT_ROOT", "output")
    reset_paths(output_root)
    print(PARAMS_BASE)
    kws = input("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„): ").split()
    top_results = search_by_keywords(kws, top_k=50)

    print("\nğŸ”  í‚¤ì›Œë“œ ì—°ê´€ ê¸°ì‚¬ Top-50")
    for rank, art in enumerate(top_results, 1):
        print(f"[{rank:02d}] ({art['score']:.3f}) {art['title']}")

    with open(RESULT_PATH, "w", encoding="utf-8") as f:
        json.dump(top_results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ…  ê²°ê³¼ â†’ {RESULT_PATH} ì €ì¥")

if __name__ == "__main__": 
    main({
        "mode": "LS2D",
        "mid": "sec",
        "sid1": "101",  # ê²½ì œ
        "sid2": "771",  # ê¸°ì—…Â·ê²½ì˜
    })
