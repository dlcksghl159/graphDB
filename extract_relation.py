import os
import json
import openai
import re
from util import merge_json, parse_json
from dotenv import load_dotenv
from deduplication import deduplicate
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Set

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
PURPOSE = os.getenv("PURPOSE", "ê¸°ì—… íŒë§¤")

# íŒ¨í„´ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œì„ ìœ„í•œ ì •ê·œì‹ íŒ¨í„´ë“¤ (PDF ì œì•ˆì‚¬í•­)
RELATION_PATTERNS = {
    "WORKS_FOR": [
        r'(\w+)(?:\s+\w+)*\s+(?:íšŒì¥|ì‚¬ì¥|ëŒ€í‘œ|ì´ì‚¬|íŒ€ì¥|ê³¼ì¥|ë¶€ì¥|ì‹¤ì¥|ì„¼í„°ì¥)',
        r'(\w+)(?:ì—ì„œ|ì˜)\s+(?:ì¼í•˜ë‹¤|ê·¼ë¬´í•˜ë‹¤|ì¬ì§í•˜ë‹¤)',
        r'(\w+)\s+(?:ì†Œì†|ì§ì›|ì„ì§ì›)'
    ],
    "CEO_OF": [
        r'(\w+)\s+(?:ëŒ€í‘œì´ì‚¬|ìµœê³ ê²½ì˜ì|CEO|ì‚¬ì¥)',
        r'(\w+)(?:ì˜)?\s+(?:ëŒ€í‘œ|ì‚¬ì¥)(?:\s+(\w+))'
    ],
    "HEADQUARTERED_IN": [
        r'(\w+)(?:ì˜)?\s+ë³¸ì‚¬(?:ëŠ”|ê°€)?\s+(\w+)(?:ì—|ì—ì„œ)',
        r'(\w+)\s+ë³¸ë¶€(?:ëŠ”|ê°€)?\s+(\w+)(?:ì—|ì—ì„œ)\s+(?:ìœ„ì¹˜|ìë¦¬)'
    ],
    "ACQUIRED": [
        r'(\w+)(?:ê°€|ì€|ëŠ”)?\s+(\w+)(?:ë¥¼|ì„)?\s+(?:ì¸ìˆ˜|ë§¤ì…|ì‚¬ë“¤ì´ë‹¤)',
        r'(\w+)(?:ì™€|ê³¼)?\s+(\w+)(?:ì˜)?\s+(?:ì¸ìˆ˜í•©ë³‘|M&A)'
    ],
    "PARTNERED_WITH": [
        r'(\w+)(?:ì™€|ê³¼)\s+(\w+)(?:ê°€|ì€|ëŠ”)?\s+(?:í˜‘ë ¥|íŒŒíŠ¸ë„ˆì‹­|ì œíœ´|í˜‘ì—…)',
        r'(\w+)(?:ì™€|ê³¼)\s+(\w+)\s+(?:í˜‘ì•½|ê³„ì•½|ì œíœ´)'
    ],
    "VISITED": [
        r'(\w+)(?:ê°€|ì€|ëŠ”)?\s+(\w+)(?:ë¥¼|ì„)?\s+(?:ë°©ë¬¸|ì°¾ì•„ê°€ë‹¤)',
        r'(\w+)\s+(\w+)\s+(?:ë°©ë¬¸|ê²¬í•™|ì‹œì°°)'
    ],
    "INVESTED_IN": [
        r'(\w+)(?:ê°€|ì€|ëŠ”)?\s+(\w+)(?:ì—|ì—ê²Œ)?\s+(?:íˆ¬ì|ì¶œì)',
        r'(\w+)\s+(\w+)\s+(?:íˆ¬ììœ ì¹˜|ìê¸ˆì¡°ë‹¬)'
    ],
    "FOUNDED": [
        r'(\w+)(?:ê°€|ì€|ëŠ”)?\s+(\w+)(?:ë¥¼|ì„)?\s+(?:ì°½ì—…|ì°½ë¦½|ì„¤ë¦½)',
        r'(\w+)\s+(?:ì°½ì—…ì|ì°½ë¦½ì|ì„¤ë¦½ì)(?:\s+(\w+))?'
    ]
}

def extract_relations_by_patterns(text: str, entities: Dict[str, str]) -> List[Dict]:
    """íŒ¨í„´ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ (PDF ì œì•ˆì‚¬í•­)"""
    extracted_relations = []
    entity_names = set(entities.keys())
    
    for relation_type, patterns in RELATION_PATTERNS.items():
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                groups = match.groups()
                
                if len(groups) >= 2:
                    entity1, entity2 = groups[0], groups[1]
                    
                    # ì—”í‹°í‹°ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    if entity1 in entity_names and entity2 in entity_names:
                        # ê´€ê³„ ë°©í–¥ ê²°ì • (ê´€ê³„ íƒ€ì…ì— ë”°ë¼)
                        if relation_type in ["WORKS_FOR", "CEO_OF"]:
                            start_node, end_node = entity1, entity2
                        elif relation_type in ["HEADQUARTERED_IN", "LOCATED_IN"]:
                            start_node, end_node = entity1, entity2
                        elif relation_type in ["VISITED"]:
                            start_node, end_node = entity1, entity2
                        else:
                            start_node, end_node = entity1, entity2
                        
                        extracted_relations.append({
                            "start_node": start_node,
                            "relationship": relation_type,
                            "end_node": end_node,
                            "properties": {
                                "extracted_by": "pattern",
                                "confidence": "high",
                                "source_text": match.group()
                            }
                        })
    
    return extracted_relations

def cross_chunk_relation_extraction(document_entities: List[Dict], document_text: str, api_key: str) -> List[Dict]:
    """í¬ë¡œìŠ¤ ì²­í¬ ê´€ê³„ ì¶”ì¶œ (PDF ì œì•ˆì‚¬í•­)"""
    if len(document_entities) < 2:
        return []
    
    client = openai.OpenAI(api_key=api_key)
    
    # ì—”í‹°í‹° ëª©ë¡ ìƒì„±
    entity_list = []
    for entity in document_entities:
        entity_list.append(f"- {entity['label']}: {entity['name']}")
    
    entity_str = "\n".join(entity_list[:50])  # ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
    
    prompt = f"""ë‹¤ìŒì€ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ì—”í‹°í‹°ë“¤ì…ë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ì—”í‹°í‹°ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

### ì—”í‹°í‹° ëª©ë¡:
{entity_str}

### ë¬¸ì„œ ì „ë¬¸:
{document_text[:4000]}  # í† í° ì œí•œ

### ì§€ì¹¨:
1. ìœ„ ì—”í‹°í‹°ë“¤ ê°„ì˜ ëª…ì‹œì /ì•”ì‹œì  ê´€ê³„ë¥¼ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”
2. ë¬¸ì„œì˜ ì—¬ëŸ¬ ë¶€ë¶„ì— ê±¸ì³ ë‚˜íƒ€ë‚˜ëŠ” ê´€ê³„ë„ í¬í•¨í•˜ì„¸ìš”  
3. ëŒ€ëª…ì‚¬ë‚˜ ì§€ì‹œì–´ë¡œ ì—°ê²°ë˜ëŠ” ê´€ê³„ë„ í•´ê²°í•˜ì„¸ìš”
4. ê° ê´€ê³„ë§ˆë‹¤ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì¥ì„ propertiesì— í¬í•¨í•˜ì„¸ìš”

### ì¶œë ¥ í˜•ì‹ (JSON):
{{
    "relations": [
        {{
            "start_node": "<ì—”í‹°í‹°ëª…>",
            "relationship": "<ê´€ê³„íƒ€ì…>",
            "end_node": "<ì—”í‹°í‹°ëª…>",
            "properties": {{
                "evidence": "<ê·¼ê±° ë¬¸ì¥>",
                "confidence": "high|medium|low"
            }}
        }}
    ]
}}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ë¬¸ì„œ ì „ì²´ë¥¼ ë¶„ì„í•˜ì—¬ ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ì°¾ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )
        
        result = parse_json(response.choices[0].message.content)
        return result.get('relations', [])
    
    except Exception as e:
        print(f"í¬ë¡œìŠ¤ ì²­í¬ ê´€ê³„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

def enhanced_process_file(filename, chunks_dir, result_dir, system_msg, nodes, api_key, document_entities_cache):
    """í–¥ìƒëœ íŒŒì¼ë³„ ê´€ê³„ ì¶”ì¶œ"""
    client = openai.OpenAI(api_key=api_key)
    i = int(filename.split('_')[-1].split('.')[0])
    
    filename_path = os.path.join(chunks_dir, filename)
    if not os.path.exists(filename_path):
        print(f"íŒŒì¼ ì—†ìŒ: {filename_path} â†’ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    with open(filename_path, "r", encoding="utf-8") as f:
        content = f.read()

    # ë…¸ë“œ ì´ë¦„ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë§¤í•‘ (íŒ¨í„´ ë§¤ì¹­ìš©)
    entity_name_to_label = {}
    for node in nodes:
        entity_name_to_label[node['name']] = node['label']

    # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ (ë” ë§ì€ ê´€ê³„ íƒ€ì…ê³¼ ì˜ˆì‹œ í¬í•¨)
    prompt = f"""ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ëª¨ë“  ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

### ì§€ì¹¨:
- ì œê³µëœ ë…¸ë“œë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš” (ìˆ˜ì • ê¸ˆì§€)
- ê´€ê³„ íƒ€ì…ì€ **SCREAMING_SNAKE_CASE**ë¡œ ì‘ì„±
- ë‹¤ì–‘í•œ ê´€ê³„ íƒ€ì…ì„ í™œìš©í•˜ì„¸ìš”:
  * ê³ ìš©: WORKS_FOR, CEO_OF, FOUNDER_OF, BOARD_MEMBER_OF
  * ìœ„ì¹˜: HEADQUARTERED_IN, LOCATED_IN, HELD_IN, LIVES_IN
  * ë¹„ì¦ˆë‹ˆìŠ¤: ACQUIRED, PARTNERED_WITH, INVESTED_IN, COMPETES_WITH, SUBSIDIARY_OF
  * ì´ë²¤íŠ¸: PARTICIPATED_IN, ATTENDED, SPOKE_AT, SPONSORED
  * ìƒí˜¸ì‘ìš©: VISITED, MET_WITH, NEGOTIATED, COLLABORATED_ON
- propertiesì—ëŠ” ë‚ ì§œ, ê¸ˆì•¡, ëª©ì  ë“± êµ¬ì²´ì  ì •ë³´ í¬í•¨
- ì•”ì‹œì  ê´€ê³„ë„ ì¶”ì¶œ (ì˜ˆ: "A íšŒì¥"ì€ A CEO_OF [íšŒì‚¬] ê´€ê³„ ì•”ì‹œ)

### ê´€ê³„ ì¶”ì¶œ ì˜ˆì‹œ:
- "ì‚¼ì„±ì „ì ì´ì¬ìš© íšŒì¥" â†’ ì´ì¬ìš© CEO_OF ì‚¼ì„±ì „ì
- "êµ¬ê¸€ê³¼ í˜‘ë ¥ ê³„ì•½" â†’ [íšŒì‚¬] PARTNERED_WITH êµ¬ê¸€  
- "ì‹¤ë¦¬ì½˜ë°¸ë¦¬ ë³¸ì‚¬ ë°©ë¬¸" â†’ [ì¸ë¬¼] VISITED [íšŒì‚¬]
- "AI ì»¨í¼ëŸ°ìŠ¤ ê¸°ì¡°ì—°ì„¤" â†’ [ì¸ë¬¼] SPOKE_AT [ì´ë²¤íŠ¸]

### ì¶œë ¥ í˜•ì‹ (JSON):
{{
    "nodes": {nodes},
    "relations": [
        {{
            "start_node": "<ì •í™•í•œì—”í‹°í‹°ì´ë¦„>",
            "relationship": "<RELATION_TYPE>",
            "end_node": "<ì •í™•í•œì—”í‹°í‹°ì´ë¦„>",
            "properties": {{
                "confidence": "high|medium|low",
                "evidence": "<ê·¼ê±° ë¬¸ì¥ ì¼ë¶€>",
                "date": "<ë‚ ì§œ>",
                "additional_info": "<ì¶”ê°€ ì •ë³´>"
            }}
        }}
    ]
}}

### í…ìŠ¤íŠ¸:
{content}

### ì—”í‹°í‹°(ë…¸ë“œ):
{nodes}"""

    # LLM ê´€ê³„ ì¶”ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
    )
    
    gpt_output = response.choices[0].message.content
    llm_result = parse_json(gpt_output)
    
    # íŒ¨í„´ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ (PDF ì œì•ˆì‚¬í•­)
    pattern_relations = extract_relations_by_patterns(content, entity_name_to_label)
    
    # LLM ê²°ê³¼ì™€ íŒ¨í„´ ê²°ê³¼ ë³‘í•©
    all_relations = llm_result.get('relations', []) + pattern_relations
    
    # ì¤‘ë³µ ì œê±° (ê°™ì€ start_node, relationship, end_node ì¡°í•©)
    seen_relations = set()
    unique_relations = []
    
    for rel in all_relations:
        rel_key = (rel['start_node'], rel['relationship'], rel['end_node'])
        if rel_key not in seen_relations:
            seen_relations.add(rel_key)
            unique_relations.append(rel)
    
    # ë¬¸ì„œë³„ ì—”í‹°í‹° ìºì‹œì— ì¶”ê°€ (í¬ë¡œìŠ¤ ì²­í¬ ê´€ê³„ ì¶”ì¶œìš©)
    doc_id = f"doc_{i // 10}"  # 10ê°œ ì²­í¬ë‹¹ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê°€ì •
    if doc_id not in document_entities_cache:
        document_entities_cache[doc_id] = {"entities": [], "text": ""}
    
    document_entities_cache[doc_id]["entities"].extend(nodes)
    document_entities_cache[doc_id]["text"] += content + "\n"
    
    final_result = {
        "nodes": nodes,
        "relations": unique_relations
    }
    
    # ê°œë³„ ê²°ê³¼ ì €ì¥
    result_path = os.path.join(result_dir, f'result_enhanced_{i}.json')
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=4)
    
    print(f"[{i}] í–¥ìƒëœ ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ - LLM: {len(llm_result.get('relations', []))}, íŒ¨í„´: {len(pattern_relations)}, ìµœì¢…: {len(unique_relations)}")

def post_process_enhanced_relations(result_json: Dict) -> Dict:
    """í–¥ìƒëœ ê´€ê³„ í›„ì²˜ë¦¬"""
    processed_relations = []
    node_names = {node['name'] for node in result_json.get('nodes', [])}
    
    # ê´€ê³„ ì •ê·œí™” ë§¤í•‘
    relation_normalization = {
        'EMPLOYED_BY': 'WORKS_FOR',
        'WORKS_AT': 'WORKS_FOR',
        'BASED_IN': 'LOCATED_IN',
        'HEADQUARTERED_AT': 'HEADQUARTERED_IN',
        'COLLABORATED_WITH': 'PARTNERED_WITH',
        'COOPERATED_WITH': 'PARTNERED_WITH'
    }
    
    dropped_relations = []
    
    for rel in result_json.get('relations', []):
        # ê´€ê³„ íƒ€ì… ì •ê·œí™”
        original_rel_type = rel['relationship']
        normalized_rel_type = relation_normalization.get(original_rel_type, original_rel_type)
        rel['relationship'] = normalized_rel_type
        
        # ë…¸ë“œ ì¡´ì¬ í™•ì¸
        if rel['start_node'] in node_names and rel['end_node'] in node_names:
            processed_relations.append(rel)
        else:
            dropped_relations.append(rel)
            print(f"âš ï¸ ê´€ê³„ ì œì™¸ (ë…¸ë“œ ì—†ìŒ): {rel['start_node']} -{rel['relationship']}-> {rel['end_node']}")
    
    result_json['relations'] = processed_relations
    
    # ë“œë¡­ëœ ê´€ê³„ í†µê³„
    if dropped_relations:
        print(f"ğŸ“Š ë“œë¡­ëœ ê´€ê³„: {len(dropped_relations)}ê°œ")
        
        # ë“œë¡­ ì›ì¸ ë¶„ì„
        missing_start = sum(1 for rel in dropped_relations if rel['start_node'] not in node_names)
        missing_end = sum(1 for rel in dropped_relations if rel['end_node'] not in node_names)
        print(f"   - start_node ëˆ„ë½: {missing_start}ê°œ")
        print(f"   - end_node ëˆ„ë½: {missing_end}ê°œ")
    
    return result_json

def main(purpose="ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„"):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
    result_dir = os.path.join(OUTPUT_ROOT, "result")
    chunks_dir = os.path.join(OUTPUT_ROOT, "chunked_document")
    os.makedirs(result_dir, exist_ok=True)

    # ë…¸ë“œ ì •ë³´ ë¡œë“œ
    with open("output/result/result.json", "r", encoding="utf-8") as f:
        result_json = json.load(f)
        nodes = result_json["nodes"]

    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")

    system_msg = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ í¬ê´„ì ì¸ ì—”í‹°í‹° ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ëª…ì‹œì  ê´€ê³„ë¿ë§Œ ì•„ë‹ˆë¼ ì•”ì‹œì  ê´€ê³„, ëŒ€ëª…ì‚¬ ì°¸ì¡°, ë¬¸ë§¥ìƒ ê´€ê³„ë„ ëª¨ë‘ ì¶”ì¶œí•©ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ê´€ê³„ íƒ€ì…ì„ í™œìš©í•˜ì—¬ í’ë¶€í•œ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""

    file_names = [f for f in os.listdir(chunks_dir) if f.endswith('.txt')]
    document_entities_cache = {}  # ë¬¸ì„œë³„ ì—”í‹°í‹° ìºì‹œ

    # 1ì°¨: ì²­í¬ë³„ ê´€ê³„ ì¶”ì¶œ
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {
            executor.submit(enhanced_process_file, filename, chunks_dir, result_dir, 
                          system_msg, nodes, api_key, document_entities_cache): filename
            for filename in file_names
        }
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="Extracting relations"):
            try:
                future.result()
            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # 2ì°¨: í¬ë¡œìŠ¤ ì²­í¬ ê´€ê³„ ì¶”ì¶œ (PDF ì œì•ˆì‚¬í•­)
    print("ğŸ”„ í¬ë¡œìŠ¤ ì²­í¬ ê´€ê³„ ì¶”ì¶œ ì‹œì‘...")
    cross_chunk_relations = []
    
    for doc_id, doc_data in document_entities_cache.items():
        if len(doc_data["entities"]) > 1:  # ì—”í‹°í‹°ê°€ 2ê°œ ì´ìƒì¸ ë¬¸ì„œë§Œ
            relations = cross_chunk_relation_extraction(
                doc_data["entities"], 
                doc_data["text"], 
                api_key
            )
            cross_chunk_relations.extend(relations)
            print(f"ğŸ“„ {doc_id}: {len(relations)}ê°œ í¬ë¡œìŠ¤ì²­í¬ ê´€ê³„ ì¶”ì¶œ")

    # 3ì°¨: ê²°ê³¼ ë³‘í•©
    result_path = os.path.join(result_dir, "result_enhanced.json")
    final_result = {"nodes": nodes, "relations": []}
    
    # ì²­í¬ë³„ ê²°ê³¼ ìˆ˜ì§‘
    for i in range(len(file_names)):
        chunked_result_path = os.path.join(result_dir, f"result_enhanced_{i}.json")
        if os.path.exists(chunked_result_path):
            with open(chunked_result_path, "r", encoding="utf-8") as f:
                chunked_data = json.load(f)
                final_result["relations"].extend(chunked_data.get("relations", []))
    
    # í¬ë¡œìŠ¤ ì²­í¬ ê´€ê³„ ì¶”ê°€
    final_result["relations"].extend(cross_chunk_relations)
    
    # ìµœì¢… í›„ì²˜ë¦¬
    final_result = post_process_enhanced_relations(final_result)
    
    # ì €ì¥
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=4)

    # ì¤‘ë³µ ì œê±°
    deduplicate(result_path)
    
    # í†µê³„ ì¶œë ¥
    print(f"âœ… í–¥ìƒëœ ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ: {result_path}")
    print(f"ğŸ“Š ì´ ê´€ê³„: {len(final_result['relations'])}ê°œ")
    print(f"ğŸ“Š í¬ë¡œìŠ¤ì²­í¬ ê´€ê³„: {len(cross_chunk_relations)}ê°œ")
    
    # ê´€ê³„ íƒ€ì…ë³„ ë¶„í¬
    from collections import Counter
    relation_types = [rel['relationship'] for rel in final_result['relations']]
    relation_stats = Counter(relation_types)
    
    print("ğŸ“ˆ ê´€ê³„ íƒ€ì… ë¶„í¬ (ìƒìœ„ 10ê°œ):")
    for rel_type, count in relation_stats.most_common(10):
        print(f"   {rel_type}: {count}ê°œ")

if __name__ == "__main__":
    main(purpose="ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„")