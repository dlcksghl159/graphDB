#!/usr/bin/env python3
"""
enhanced_main_pipeline.py
========================
í–¥ìƒëœ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

PDF ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
ê° ë‹¨ê³„ë³„ í’ˆì§ˆ ê²€ì¦ê³¼ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# ê°œì„ ëœ ëª¨ë“ˆë“¤ import
try:
    from enhanced_schema_multi import main as extract_enhanced_schema
    from enhanced_extract_node_kr import main as extract_enhanced_nodes
    from enhanced_extract_relation_kr import main as extract_enhanced_relations
    from enhanced_deduplication_validation import main_enhanced_deduplication
    from cypher import main as generate_cypher
    from send_cypher import main as send_cypher
    from enhanced_rag import enhanced_answer, analyze_entity_connections
except ImportError as e:
    print(f"âš ï¸ ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")
    print("ê°œì„ ëœ ëª¨ë“ˆ íŒŒì¼ë“¤ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

class PipelineMonitor:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, output_root: str):
        self.output_root = Path(output_root)
        self.start_time = time.time()
        self.steps = {}
        self.log_file = self.output_root / "pipeline_log.json"
        
    def start_step(self, step_name: str, description: str = ""):
        """ë‹¨ê³„ ì‹œì‘ ê¸°ë¡"""
        self.steps[step_name] = {
            "description": description,
            "start_time": time.time(),
            "end_time": None,
            "duration": 0,
            "status": "running",
            "metrics": {},
            "errors": []
        }
        print(f"ğŸš€ {step_name}: {description}")
        
    def end_step(self, step_name: str, status: str = "success", metrics: Dict = None, errors: List = None):
        """ë‹¨ê³„ ì™„ë£Œ ê¸°ë¡"""
        if step_name in self.steps:
            self.steps[step_name]["end_time"] = time.time()
            self.steps[step_name]["duration"] = self.steps[step_name]["end_time"] - self.steps[step_name]["start_time"]
            self.steps[step_name]["status"] = status
            self.steps[step_name]["metrics"] = metrics or {}
            self.steps[step_name]["errors"] = errors or []
            
            duration = self.steps[step_name]["duration"]
            status_emoji = "âœ…" if status == "success" else "âŒ" if status == "error" else "âš ï¸"
            print(f"{status_emoji} {step_name} ì™„ë£Œ ({duration:.1f}ì´ˆ)")
            
            if metrics:
                for key, value in metrics.items():
                    print(f"   ğŸ“Š {key}: {value}")
    
    def save_log(self):
        """ë¡œê·¸ ì €ì¥"""
        log_data = {
            "pipeline_start": self.start_time,
            "pipeline_end": time.time(),
            "total_duration": time.time() - self.start_time,
            "timestamp": datetime.now().isoformat(),
            "steps": self.steps
        }
        
        with open(self.log_file, "w", encoding="utf-8") as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ì‹¤í–‰ ë¡œê·¸ ì €ì¥: {self.log_file}")

def validate_environment():
    """í™˜ê²½ ì„¤ì • ê²€ì¦"""
    required_env_vars = ["OPENAI_API_KEY", "NEO4J_URI", "NEO4J_USERNAME", "NEO4J_PASSWORD"]
    missing_vars = []
    
    for var in required_env_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"âŒ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {missing_vars}")
        return False
    
    print("âœ… í™˜ê²½ ì„¤ì • ê²€ì¦ ì™„ë£Œ")
    return True

def check_input_files(chunks_dir: Path) -> Dict[str, Any]:
    """ì…ë ¥ íŒŒì¼ ê²€ì¦"""
    if not chunks_dir.exists():
        return {"status": "error", "message": f"ì²­í¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {chunks_dir}"}
    
    chunk_files = list(chunks_dir.glob("chunked_output_*.txt"))
    if not chunk_files:
        return {"status": "error", "message": "ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"}
    
    total_size = sum(f.stat().st_size for f in chunk_files)
    
    return {
        "status": "success",
        "file_count": len(chunk_files),
        "total_size_mb": total_size / (1024 * 1024),
        "files": [f.name for f in chunk_files[:5]]  # ì²˜ìŒ 5ê°œë§Œ
    }

def analyze_extraction_quality(result_file: Path) -> Dict[str, Any]:
    """ì¶”ì¶œ í’ˆì§ˆ ë¶„ì„"""
    if not result_file.exists():
        return {"status": "error", "message": "ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"}
    
    try:
        with open(result_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        nodes = data.get("nodes", [])
        relations = data.get("relations", [])
        
        # ë…¸ë“œ ë¶„ì„
        node_types = {}
        nodes_with_aliases = 0
        nodes_with_properties = 0
        
        for node in nodes:
            label = node.get("label", "UNKNOWN")
            node_types[label] = node_types.get(label, 0) + 1
            
            props = node.get("properties", {})
            if "aliases" in props:
                nodes_with_aliases += 1
            if len(props) > 0:
                nodes_with_properties += 1
        
        # ê´€ê³„ ë¶„ì„
        relation_types = {}
        for rel in relations:
            rel_type = rel.get("relationship", "UNKNOWN")
            relation_types[rel_type] = relation_types.get(rel_type, 0) + 1
        
        # ì—°ê²°ì„± ë¶„ì„
        referenced_nodes = set()
        for rel in relations:
            referenced_nodes.add(rel.get("start_node"))
            referenced_nodes.add(rel.get("end_node"))
        
        connectivity = len(referenced_nodes) / len(nodes) if nodes else 0
        
        return {
            "status": "success",
            "total_nodes": len(nodes),
            "total_relations": len(relations),
            "node_types": node_types,
            "relation_types": relation_types,
            "nodes_with_aliases": nodes_with_aliases,
            "nodes_with_properties": nodes_with_properties,
            "connectivity_score": connectivity,
            "orphaned_nodes": len(nodes) - len(referenced_nodes)
        }
    
    except Exception as e:
        return {"status": "error", "message": str(e)}

def run_enhanced_pipeline(purpose: str = "ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„", skip_steps: List[str] = None):
    """í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    skip_steps = skip_steps or []
    OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
    monitor = PipelineMonitor(OUTPUT_ROOT)
    
    # í™˜ê²½ ê²€ì¦
    if not validate_environment():
        return False
    
    # ì…ë ¥ íŒŒì¼ ê²€ì¦
    chunks_dir = Path(OUTPUT_ROOT) / "chunked_document"
    input_check = check_input_files(chunks_dir)
    if input_check["status"] == "error":
        print(f"âŒ ì…ë ¥ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {input_check['message']}")
        return False
    
    print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {input_check['file_count']}ê°œ ({input_check['total_size_mb']:.1f}MB)")
    
    try:
        # 1ë‹¨ê³„: í–¥ìƒëœ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
        if "schema" not in skip_steps:
            monitor.start_step("enhanced_schema", "í™•ì¥ëœ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ")
            try:
                extract_enhanced_schema(purpose)
                
                # ìŠ¤í‚¤ë§ˆ í’ˆì§ˆ ê²€ì¦
                schema_file = Path(OUTPUT_ROOT) / "schema" / "schema.json"
                if schema_file.exists():
                    with open(schema_file, "r", encoding="utf-8") as f:
                        schema_data = json.load(f)
                    
                    schema_metrics = {
                        "node_types": len(schema_data.get("nodes", [])),
                        "relation_types": len(schema_data.get("relations", [])),
                        "file_size_kb": schema_file.stat().st_size / 1024
                    }
                    monitor.end_step("enhanced_schema", "success", schema_metrics)
                else:
                    monitor.end_step("enhanced_schema", "error", errors=["ìŠ¤í‚¤ë§ˆ íŒŒì¼ ìƒì„± ì‹¤íŒ¨"])
                    
            except Exception as e:
                monitor.end_step("enhanced_schema", "error", errors=[str(e)])
                raise
        
        # 2ë‹¨ê³„: í–¥ìƒëœ ë…¸ë“œ ì¶”ì¶œ
        if "nodes" not in skip_steps:
            monitor.start_step("enhanced_nodes", "í–¥ìƒëœ ì—”í‹°í‹° ì¶”ì¶œ (NER í´ë°± í¬í•¨)")
            try:
                extract_enhanced_nodes(purpose)
                
                # ë…¸ë“œ í’ˆì§ˆ ë¶„ì„
                result_file = Path(OUTPUT_ROOT) / "result" / "result.json"
                node_quality = analyze_extraction_quality(result_file)
                if node_quality["status"] == "success":
                    monitor.end_step("enhanced_nodes", "success", node_quality)
                else:
                    monitor.end_step("enhanced_nodes", "warning", errors=[node_quality["message"]])
                    
            except Exception as e:
                monitor.end_step("enhanced_nodes", "error", errors=[str(e)])
                raise
        
        # 3ë‹¨ê³„: í–¥ìƒëœ ê´€ê³„ ì¶”ì¶œ
        if "relations" not in skip_steps:
            monitor.start_step("enhanced_relations", "í™•ì¥ëœ ê´€ê³„ ì¶”ì¶œ (íŒ¨í„´ ë§¤ì¹­ + í¬ë¡œìŠ¤ì²­í¬)")
            try:
                extract_enhanced_relations(purpose)
                
                # ê´€ê³„ í’ˆì§ˆ ë¶„ì„
                enhanced_result_file = Path(OUTPUT_ROOT) / "result" / "result_enhanced.json"
                if enhanced_result_file.exists():
                    relation_quality = analyze_extraction_quality(enhanced_result_file)
                    if relation_quality["status"] == "success":
                        monitor.end_step("enhanced_relations", "success", relation_quality)
                    else:
                        monitor.end_step("enhanced_relations", "warning", errors=[relation_quality["message"]])
                else:
                    monitor.end_step("enhanced_relations", "error", errors=["ê´€ê³„ ì¶”ì¶œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨"])
                    
            except Exception as e:
                monitor.end_step("enhanced_relations", "error", errors=[str(e)])
                raise
        
        # 4ë‹¨ê³„: í–¥ìƒëœ ì¤‘ë³µ ì œê±° ë° ê²€ì¦
        if "deduplication" not in skip_steps:
            monitor.start_step("enhanced_dedup", "ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ ê²€ì¦")
            try:
                result_file = Path(OUTPUT_ROOT) / "result" / "result.json"
                
                # ì¤‘ë³µ ì œê±° ì „ í’ˆì§ˆ ì¸¡ì •
                pre_quality = analyze_extraction_quality(result_file)
                
                main_enhanced_deduplication(str(result_file))
                
                # ì¤‘ë³µ ì œê±° í›„ í’ˆì§ˆ ì¸¡ì •
                post_quality = analyze_extraction_quality(result_file)
                
                if pre_quality["status"] == "success" and post_quality["status"] == "success":
                    dedup_metrics = {
                        "nodes_before": pre_quality["total_nodes"],
                        "nodes_after": post_quality["total_nodes"],
                        "relations_before": pre_quality["total_relations"],
                        "relations_after": post_quality["total_relations"],
                        "connectivity_improvement": post_quality["connectivity_score"] - pre_quality["connectivity_score"],
                        "orphaned_reduction": pre_quality["orphaned_nodes"] - post_quality["orphaned_nodes"]
                    }
                    monitor.end_step("enhanced_dedup", "success", dedup_metrics)
                else:
                    monitor.end_step("enhanced_dedup", "error", errors=["í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨"])
                    
            except Exception as e:
                monitor.end_step("enhanced_dedup", "error", errors=[str(e)])
                raise
        
        # 5ë‹¨ê³„: Cypher ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        if "cypher" not in skip_steps:
            monitor.start_step("cypher_generation", "Neo4j Cypher ìŠ¤í¬ë¦½íŠ¸ ìƒì„±")
            try:
                generate_cypher()
                
                cypher_file = Path(OUTPUT_ROOT) / "graph.cypher"
                if cypher_file.exists():
                    cypher_metrics = {
                        "file_size_kb": cypher_file.stat().st_size / 1024,
                        "line_count": len(cypher_file.read_text(encoding="utf-8").splitlines())
                    }
                    monitor.end_step("cypher_generation", "success", cypher_metrics)
                else:
                    monitor.end_step("cypher_generation", "error", errors=["Cypher íŒŒì¼ ìƒì„± ì‹¤íŒ¨"])
                    
            except Exception as e:
                monitor.end_step("cypher_generation", "error", errors=[str(e)])
                raise
        
        # 6ë‹¨ê³„: Neo4j ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©
        if "neo4j" not in skip_steps:
            monitor.start_step("neo4j_loading", "Neo4j ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”© ë° ì„ë² ë”©")
            try:
                # Neo4j ë¡œë”©ì€ ë³„ë„ í•¨ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ send_cypher ëª¨ë“ˆ ì‹¤í–‰
                import subprocess
                result = subprocess.run([sys.executable, "send_cypher.py"], 
                                      capture_output=True, text=True)
                
                if result.returncode == 0:
                    neo4j_metrics = {
                        "loading_success": True,
                        "stdout_lines": len(result.stdout.splitlines()),
                        "stderr_lines": len(result.stderr.splitlines())
                    }
                    monitor.end_step("neo4j_loading", "success", neo4j_metrics)
                else:
                    monitor.end_step("neo4j_loading", "error", errors=[result.stderr])
                    
            except Exception as e:
                monitor.end_step("neo4j_loading", "error", errors=[str(e)])
                raise
        
        # ìµœì¢… í’ˆì§ˆ ë¦¬í¬íŠ¸
        monitor.start_step("final_report", "ìµœì¢… í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±")
        try:
            final_result_file = Path(OUTPUT_ROOT) / "result" / "result.json"
            final_quality = analyze_extraction_quality(final_result_file)
            
            if final_quality["status"] == "success":
                print("\n" + "="*60)
                print("ğŸ“Š ìµœì¢… ì§€ì‹ ê·¸ë˜í”„ í’ˆì§ˆ ë¦¬í¬íŠ¸")
                print("="*60)
                print(f"ì´ ë…¸ë“œ: {final_quality['total_nodes']:,}ê°œ")
                print(f"ì´ ê´€ê³„: {final_quality['total_relations']:,}ê°œ")
                print(f"ì—°ê²°ì„± ì ìˆ˜: {final_quality['connectivity_score']:.3f}")
                print(f"ê³ ë¦½ëœ ë…¸ë“œ: {final_quality['orphaned_nodes']}ê°œ")
                print(f"ë³„ì¹­ ë³´ìœ  ë…¸ë“œ: {final_quality['nodes_with_aliases']}ê°œ")
                print(f"ì†ì„± ë³´ìœ  ë…¸ë“œ: {final_quality['nodes_with_properties']}ê°œ")
                
                print("\nğŸ“ˆ ë…¸ë“œ íƒ€ì… ë¶„í¬:")
                for node_type, count in sorted(final_quality['node_types'].items(), key=lambda x: x[1], reverse=True):
                    print(f"   {node_type}: {count}ê°œ")
                
                print("\nğŸ“ˆ ê´€ê³„ íƒ€ì… ë¶„í¬ (ìƒìœ„ 10ê°œ):")
                sorted_relations = sorted(final_quality['relation_types'].items(), key=lambda x: x[1], reverse=True)
                for rel_type, count in sorted_relations[:10]:
                    print(f"   {rel_type}: {count}ê°œ")
                
                monitor.end_step("final_report", "success", final_quality)
            else:
                monitor.end_step("final_report", "error", errors=[final_quality["message"]])
                
        except Exception as e:
            monitor.end_step("final_report", "error", errors=[str(e)])
        
        # ë¡œê·¸ ì €ì¥
        monitor.save_log()
        
        total_duration = time.time() - monitor.start_time
        print(f"\nğŸ‰ í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ (ì´ {total_duration:.1f}ì´ˆ)")
        return True
        
    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        monitor.save_log()
        return False

def quick_test_rag(test_questions: List[str] = None):
    """RAG ì‹œìŠ¤í…œ ê°„ë‹¨ í…ŒìŠ¤íŠ¸"""
    if test_questions is None:
        test_questions = [
            "êµ¬ê¸€ì—ì„œ ì¼í•˜ëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€ìš”?",
            "ì‚¼ì„±ì „ìì˜ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
            "ë„¤ì´ë²„ì™€ í˜‘ë ¥í•˜ëŠ” íšŒì‚¬ëŠ” ì–´ë–¤ ê³³ë“¤ì´ ìˆë‚˜ìš”?",
            "AI ê¸°ìˆ ì„ ê°œë°œí•˜ëŠ” íšŒì‚¬ë“¤ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "ìµœê·¼ì— íˆ¬ìë°›ì€ íšŒì‚¬ëŠ” ì–´ë””ì¸ê°€ìš”?"
        ]
    
    print("\n" + "="*50)
    print("ğŸ§ª RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n[{i}/{len(test_questions)}] ì§ˆë¬¸: {question}")
        
        start_time = time.time()
        try:
            answer = enhanced_answer(question)
            duration = time.time() - start_time
            
            print(f"ë‹µë³€: {answer}")
            print(f"ì‘ë‹µì‹œê°„: {duration:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í–¥ìƒëœ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--purpose", default="ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„", help="ë¶„ì„ ëª©ì ")
    parser.add_argument("--skip", nargs="*", default=[], 
                       choices=["schema", "nodes", "relations", "deduplication", "cypher", "neo4j"],
                       help="ê±´ë„ˆë›¸ ë‹¨ê³„ë“¤")
    parser.add_argument("--test-rag", action="store_true", help="RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--quick-test", action="store_true", help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ í–¥ìƒëœ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print(f"ğŸ“‹ ë¶„ì„ ëª©ì : {args.purpose}")
    if args.skip:
        print(f"â­ï¸ ê±´ë„ˆë›¸ ë‹¨ê³„: {args.skip}")
    
    if args.quick_test:
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
        quick_test_rag()
        return
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    success = run_enhanced_pipeline(args.purpose, args.skip)
    
    if success and args.test_rag:
        # RAG í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        quick_test_rag()

if __name__ == "__main__":
    main()