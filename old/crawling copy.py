# filename: news_keyword_search.py
"""ë„¤ì´ë²„ ê²½ì œÂ·ê¸°ì—… ì„¹ì…˜ ê¸°ì‚¬ Nê°œë¥¼ í¬ë¡¤ë§í•´ ì„ë² ë”©Â·FAISS ì¸ë±ìŠ¤ë¥¼ ë§Œë“  ë’¤
   í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥/ì €ì¥í•œë‹¤. ëª¨ë“  ì‚°ì¶œë¬¼ì€ ./output/news í´ë”ì— ì €ì¥ëœë‹¤.
   â”€ ì—…ë°ì´íŠ¸: 2025â€‘05â€‘12
"""

import os
import json
import time
from datetime import datetime
from typing import List, Tuple

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv

# ---------------------- 0. í™˜ê²½ ë° ê²½ë¡œ ì„¤ì • ----------------------
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY:
    raise RuntimeError("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”!")

client = OpenAI(api_key=API_KEY)

# ì‚°ì¶œë¬¼ì„ ì €ì¥í•  í´ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")   # ê¸°ë³¸ê°’ "output"
OUTPUT_DIR = os.path.join(OUTPUT_ROOT, "news")
os.makedirs(OUTPUT_DIR, exist_ok=True)

TOP_N = 100                       # í¬ë¡¤ë§Â·ì„ë² ë”©í•  ê¸°ì‚¬ ìˆ˜
EMB_MODEL = "text-embedding-3-small"  # 256/512ì°¨ì›

# íŒŒì¼ ê²½ë¡œ ---------------------------------------------------------
JSON_PATH = f"{OUTPUT_DIR}/naver_news_top{TOP_N}.json"
IDX_PATH  = f"{OUTPUT_DIR}/news_index_{TOP_N}.faiss"
VEC_PATH  = f"{OUTPUT_DIR}/news_vectors_{TOP_N}.npy"
RESULT_PATH = f"{OUTPUT_DIR}/keyword_results.json"

# ---------------------- 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ----------------------
SECTION_URL = "https://news.naver.com/main/list.naver"
PARAMS_BASE = {
    "mode": "LS2D",
    "mid": "sec",
    "sid1": "101",  # ê²½ì œ
    "sid2": "771",  # ê¸°ì—…Â·ê²½ì˜
}
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://www.naver.com",
}

def reset_paths(output_root: str):
    if not output_root:
        output_root = "output"
    global OUTPUT_ROOT, OUTPUT_DIR, JSON_PATH, IDX_PATH, VEC_PATH, RESULT_PATH
    OUTPUT_ROOT = output_root
    OUTPUT_DIR  = os.path.join(OUTPUT_ROOT, "news")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    JSON_PATH   = f"{OUTPUT_DIR}/naver_news_top{TOP_N}.json"
    IDX_PATH    = f"{OUTPUT_DIR}/news_index_{TOP_N}.faiss"
    VEC_PATH    = f"{OUTPUT_DIR}/news_vectors_{TOP_N}.npy"
    RESULT_PATH = f"{OUTPUT_DIR}/keyword_results.json"

def crawl_top_n(n: int = TOP_N) -> List[dict]:
    """ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ ê¸°ì—…Â·ê²½ì˜ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ nê°œ ìˆ˜ì§‘"""
    print(f"ğŸ“°  ë„¤ì´ë²„ ê¸°ì‚¬ {n}ê°œ í¬ë¡¤ë§ ì¤‘â€¦")
    links, page = [], 1
    today = datetime.now().strftime("%Y%m%d")

    # â‘  ë§í¬ ìˆ˜ì§‘
    while len(links) < n:
        params = PARAMS_BASE | {"page": str(page), "date": today}
        res = requests.get(SECTION_URL, headers=HEADERS, params=params, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        new_links = [
            ("https://news.naver.com" + a["href"] if a["href"].startswith("/") else a["href"])
            for a in soup.select("div.list_body.newsflash_body ul li dl dt a")
            if (txt := a.get_text(strip=True)) and txt != "ë™ì˜ìƒê¸°ì‚¬"
        ]
        if not new_links:
            break  # ë” ì´ìƒ ê¸°ì‚¬ ì—†ìŒ
        for l in new_links:
            if l not in links:
                links.append(l)
                if len(links) >= n:
                    break
        page += 1
        time.sleep(0.1)

    # â‘¡ ë³¸ë¬¸ íŒŒì‹±
    def parse(url: str) -> dict:
        r = requests.get(url, headers=HEADERS, timeout=10)
        r.raise_for_status()
        s = BeautifulSoup(r.text, "html.parser")

        title_tag = (
            s.select_one("#articleTitle")
            or s.select_one("h2.media_end_head_headline")
            or s.select_one("h2.media_end_head_title")
            or s.select_one("h3.end_tit")
        )
        title = title_tag.get_text(strip=True) if title_tag else ""

        body_tag = (
            s.select_one("#articleBodyContents")
            or s.select_one("div#newsct_article._article_body")
            or s.select_one("div#newsEndContents")
        )
        content = ""
        if body_tag:
            for t in body_tag.select(
                "script, .end_photo_org, .link_news, span.media_end_linked_more_url"
            ):
                t.decompose()
            content = body_tag.get_text("\n", strip=True)
        return {"title": title, "content": content}

    # â‘¢ ê¸°ì‚¬ ìƒì„¸ íŒŒì‹±
    articles = []
    for i, url in enumerate(links, 1):
        try:
            art = parse(url)
            articles.append(art)
            print(f"[{i:02d}/{n}] {art['title'][:60]}â€¦")
        except Exception as e:
            print(f"  â†’ ì‹¤íŒ¨({e})")

    # â‘£ JSON ì €ì¥
    with open(JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ…  í¬ë¡¤ë§ ì™„ë£Œ â†’ {JSON_PATH} ì €ì¥\n")
    return articles

# ---------------------- 2. ì„ë² ë”© & ì¸ë±ìŠ¤ ----------------------

def get_embedding(text: str) -> List[float]:
    text = text.replace("\n", " ")
    while True:
        try:
            emb = client.embeddings.create(input=[text], model=EMB_MODEL).data[0].embedding
            return emb
        except Exception as e:
            print("  â³ ì¬ì‹œë„:", e)
            time.sleep(2)


def build_or_load_index() -> Tuple[faiss.IndexFlatIP, List[dict]]:
    """ê¸°ì‚¬ JSON â†’ ì„ë² ë”© â†’ FAISS ì¸ë±ìŠ¤. ìºì‹œ í™œìš©"""
    if not os.path.exists(JSON_PATH):  # JSON ì—†ìœ¼ë©´ í¬ë¡¤ë§
        crawl_top_n(TOP_N)

    with open(JSON_PATH, encoding="utf-8") as f:
        articles: List[dict] = json.load(f)

    if len(articles) < TOP_N:  # ê¸°ì‚¬ ë¶€ì¡± ì‹œ ì¬í¬ë¡¤ë§
        articles = crawl_top_n(TOP_N)

    # ìºì‹œëœ ë²¡í„°Â·ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(IDX_PATH) and os.path.exists(VEC_PATH):
        xb = np.load(VEC_PATH).astype("float32")
        dim = xb.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(xb)
        return index, articles

    # ìƒˆë¡œ ì„ë² ë”© ìƒì„±
    print("\nğŸ§®  ì„ë² ë”© ìƒì„± ì¤‘â€¦")
    vectors = []
    for art in tqdm(articles, desc="ì„ë² ë”©"):
        txt = (art["title"] + "\n\n" + art["content"])[:3500]
        vec = np.asarray(get_embedding(txt), dtype=np.float32)
        vec /= np.linalg.norm(vec) + 1e-10
        vectors.append(vec)

    xb = np.vstack(vectors).astype("float32")
    np.save(VEC_PATH, xb)

    dim = xb.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(xb)
    faiss.write_index(index, IDX_PATH)
    print("âœ…  ì„ë² ë”© & ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ\n")
    return index, articles

# ---------------------- 3. í‚¤ì›Œë“œ ê²€ìƒ‰ ----------------------

def search_by_keywords(keywords: List[str], top_k: int = 50) -> List[dict]:
    query = " ".join(keywords)
    q_vec = np.asarray(get_embedding(query), dtype=np.float32)
    q_vec /= np.linalg.norm(q_vec) + 1e-10

    index, articles = build_or_load_index()
    D, I = index.search(q_vec[None, :], top_k)

    results = []
    for score, idx in zip(D[0], I[0]):
        art = articles[int(idx)].copy()
        art["score"] = float(score)
        results.append(art)
    return results

# ---------------------- 4. ì‹¤í–‰ ----------------------
def main(params_base):
    global PARAMS_BASE
    PARAMS_BASE = params_base
    output_root = os.getenv("OUTPUT_ROOT", "output")
    reset_paths(output_root)
    print(PARAMS_BASE)
    kws = input("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„): ").split()
    top_results = search_by_keywords(kws, top_k=50)

    print("\nğŸ”  í‚¤ì›Œë“œ ì—°ê´€ ê¸°ì‚¬ Top-50")
    for rank, art in enumerate(top_results, 1):
        print(f"[{rank:02d}] ({art['score']:.3f}) {art['title']}")

    with open(RESULT_PATH, "w", encoding="utf-8") as f:
        json.dump(top_results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ…  ê²°ê³¼ â†’ {RESULT_PATH} ì €ì¥")

if __name__ == "__main__": 
    main({
        "mode": "LS2D",
        "mid": "sec",
        "sid1": "101",  # ê²½ì œ
        "sid2": "771",  # ê¸°ì—…Â·ê²½ì˜
    })
