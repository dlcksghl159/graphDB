import os
import json
import hashlib
import requests
import openai

# api_key = "up_lQV4sikxTIxBWw1mhqDec6WI6cu49"  
# filename = "./obzen.pdf"  # íŒŒì¼ ê²½ë¡œ
# # filename = "./fake_document.pdf"
# output_file = "parsed_result.json"  # ì €ì¥í•  JSON íŒŒì¼ ì´ë¦„

# url = "https://api.upstage.ai/v1/document-digitization"
# headers = {"Authorization": f"Bearer {api_key}"}
# files = {"document": open(filename, "rb")}
# data = {"ocr": "force", "model": "document-parse"}

# response = requests.post(url, headers=headers, files=files, data=data)

# # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(response.json(), f, ensure_ascii=False, indent=2)

# print(f"âœ… ê²°ê³¼ê°€ '{output_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# #----------------------------------------------------------------------------------
# # 1. JSON íŒŒì¼ ë¡œë”©
# with open("parsed_result.json", "r", encoding="utf-8") as f:
#     data = json.load(f)

# # 2. HTML ë¶€ë¶„ë§Œ ì¶”ì¶œ
# html_content = data["content"].get("html", "")

# # 3. í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
# with open("html_content.txt", "w", encoding="utf-8") as f:
#     f.write(html_content)

# # -----------------------------------------------------------------------------------
# #ë¬¸ì„œ chunking 
# from bs4 import BeautifulSoup
# import re

# def split_html_by_sentence(html: str, chunk_size: int = 3000) -> list[str]:
#     soup = BeautifulSoup(html, "html.parser")
    
#     # í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ HTML ê·¸ëŒ€ë¡œ ìœ ì§€
#     blocks = []
#     for tag in soup.find_all(["h1", "h2", "h3", "p", "table", "ul", "ol", "figure"]):
#         blocks.append(str(tag))

#     # ë¬¸ì¥ ê¸°ì¤€ ì²­í¬ ë‚˜ëˆ„ê¸°
#     chunks = []
#     current_chunk = ""

#     sentence_end_pattern = re.compile(r"(?<=[.!?ã€‚])\s|(?<=</p>)|(?<=</table>)")

#     for block in blocks:
#         current_chunk += block

#         if len(current_chunk) >= chunk_size:
#             # ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
#             split_points = sentence_end_pattern.split(current_chunk)
#             temp_chunk = ""
#             for part in split_points:
#                 temp_chunk += part
#                 if len(temp_chunk) >= chunk_size:
#                     chunks.append(temp_chunk.strip())
#                     temp_chunk = ""
#             current_chunk = temp_chunk

#     if current_chunk.strip():
#         chunks.append(current_chunk.strip())

#     return chunks

# # ì‚¬ìš© ì˜ˆì‹œ
# with open("html_content.txt", "r", encoding="utf-8") as f:
#     html_text = f.read()

# chunks = split_html_by_sentence(html_text)

# for i, chunk in enumerate(chunks):
#     print(f"\n### Chunk {i+1} ###\n{chunk}\n")  # ì•ë¶€ë¶„ë§Œ ì¶œë ¥

# # docsë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
# for i, chunk in enumerate(chunks):
#     with open(f"chunked_output_{i}.txt", "w", encoding="utf-8") as f:
#         f.write(f"{chunk}")



# # --------------------------------------------------------------------------------


import os
purpose = input('ëª©ì  ì…ë ¥: ')

system_msg = (
    "You extract entity/relation schemas from text for knowledge graphs used in RAG systems. "
    "Respond with valid JSON only."
)

client = openai.OpenAI(api_key="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")

from collections import defaultdict

def merge_schema_with_relations(old_schema: dict, new_schema: dict) -> dict:
    # --- ë…¸ë“œ ë³‘í•© ---
    def build_node_dict(nodes):
        result = {}
        for node in nodes:
            label = node["label"]
            properties = node.get("properties", {})
            if label in result:
                result[label].update(properties)
            else:
                result[label] = properties.copy()
        return result

    old_nodes = build_node_dict(old_schema.get("nodes", []))
    new_nodes = build_node_dict(new_schema.get("nodes", []))
    all_labels = set(old_nodes.keys()).union(new_nodes.keys())

    merged_nodes = []
    for label in all_labels:
        merged_props = {**old_nodes.get(label, {}), **new_nodes.get(label, {})}
        merged_nodes.append({
            "label": label,
            "properties": merged_props
        })

    # --- ê´€ê³„ ë³‘í•© ---
    def rel_key(rel):
        return (rel["start_node"], rel["relationship"], rel["end_node"])

    old_rels_dict = {rel_key(r): r.get("properties", {}) for r in old_schema.get("relations", [])}
    new_rels_dict = {rel_key(r): r.get("properties", {}) for r in new_schema.get("relations", [])}
    all_keys = set(old_rels_dict.keys()).union(new_rels_dict.keys())

    merged_relations = []
    for key in all_keys:
        start_node, relationship, end_node = key
        merged_props = {**old_rels_dict.get(key, {}), **new_rels_dict.get(key, {})}
        merged_relations.append({
            "start_node": start_node,
            "relationship": relationship,
            "end_node": end_node,
            "properties": merged_props
        })

    return {
        "nodes": merged_nodes,
        "relations": merged_relations
    }



i = 0
while True:
    filename = f"chunked_output_{i}.txt"
    
    if not os.path.exists(filename):
        print(f"ğŸ“ íŒŒì¼ ì—†ìŒ: {filename} â†’ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    with open(filename, "r", encoding="utf-8") as f:
        content = f.read()

    # GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f'''{content}\n\nGive me Json Schema of Node types and Relations types 
    to make a knowledge graph for RAG system '{purpose}', 
    using this text, in the form of  
    { 
        {"nodes": [{"label":"", "properties":{}},] },  
        { "relations": [{"start_node":"", "relationship":"", "end_node":"", "properties":{}}, ]}
    }.  You just have to decide keys of properties and for each property keys you should put "" as value.'''

    
    # OpenAI GPT í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
    )

    # ê²°ê³¼ ì¶œë ¥
    print("ğŸ“„ GPT ì‘ë‹µ:")
    print(response.choices[0].message.content)

    # ì‘ë‹µ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    gpt_output = response.choices[0].message.content

    import re

    # GPT ì‘ë‹µ ì¤‘ JSON êµ¬ì¡°ë§Œ ì¶”ì¶œ (Lazy match, ì „ì²´ ë‚´ìš©)
    match = re.search(r'\{\s*"nodes"\s*:\s*\[.*?\],\s*"relations"\s*:\s*\[.*?\]\s*\}', gpt_output, re.DOTALL)
    if match:
        json_data = match.group()
        try:
            parsed_json = json.loads(json_data)
        except json.JSONDecodeError as e:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
            parsed_json = {}
    else:
        print("âŒ JSON êµ¬ì¡°ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ")
        parsed_json = {}

    schema_path = "schema.json"

    with open(schema_path+f'_{i}.json', "w", encoding="utf-8") as f:
            json.dump(parsed_json, f, ensure_ascii=False, indent=4)

    

    if not os.path.exists(schema_path):
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± + parsed_json ì €ì¥
        with open(schema_path, "w", encoding="utf-8") as f:
            json.dump(parsed_json, f, ensure_ascii=False, indent=4)
        print("âœ… schema.jsonì´ ì—†ì–´ì„œ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        old_schema = None
    else:
        # íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
        with open(schema_path, "r", encoding="utf-8") as f:
            old_schema = json.load(f)
        print("ğŸ“„ ê¸°ì¡´ schema.jsonì„ old_schemaë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        merged_schema = merge_schema_with_relations(old_schema, parsed_json)
        print("old schema")
        print(old_schema)
        print("schema")
        print(parsed_json)
        print("new schema")
        print(merged_schema)
        with open(schema_path, "w", encoding="utf-8") as f:
            json.dump(merged_schema, f, ensure_ascii=False, indent=4)
        print("âœ… ë³‘í•©ëœ schema ì €ì¥ ì™„ë£Œ: schema.json")

    
    i += 1






# --------------------------------------------------------------------------
# 1. JSON íŒŒì¼ ë¡œë”©
with open("schema.json", "r", encoding="utf-8") as f:
    schema_json = json.load(f)

purpose = input('ëª©ì  ì…ë ¥ : ')

system_msg = (
    "You extract entity/relation from text for knowledge graphs used in RAG systems. "
    "Respond with valid JSON only."
)


client = openai.OpenAI(api_key="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")


i = 0
while True:
    filename = f"chunked_output_{i}.txt"
    
    if not os.path.exists(filename):
        print(f"ğŸ“ íŒŒì¼ ì—†ìŒ: {filename} â†’ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    with open(filename, "r", encoding="utf-8") as f:
        content = f.read()
        print(f"âœ… chunked_output_{i}.txt ë‚´ìš©:\n{content}\n")

    prompt = f'''
    Text : {content}, 
    Schema : {schema_json}, 
    Extract specific nodes and relations in JSON format from the given text using the given schema for a knowledge graph 
    to be used in a RAG system focused on '{purpose}'. 
    '''

    
    # OpenAI GPT í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
    )

    
    # ê²°ê³¼ ì¶œë ¥
    print("ğŸ“„ GPT ì‘ë‹µ:")
    print(response.choices[0].message.content)

        
    # ì‘ë‹µ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    gpt_output = response.choices[0].message.content

    import re


    # JSON êµ¬ì¡° ê°ì§€ ë° ì¶”ì¶œ (ì „ì²´ JSON ë¸”ë¡ì„ greedyí•˜ê²Œ íƒì§€)
    match = re.search(r'\{\s*"nodes"\s*:\s*\[.*\],\s*"relations"\s*:\s*\[.*\]\s*\}', gpt_output, re.DOTALL)

    parsed_json = {}

    if match:
        json_data = match.group()
        
        # JSON ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤„ ë°”ê¿ˆ ì œê±° ê°€ëŠ¥)
        json_data = json_data.strip()

        try:
            # JSON ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±
            parsed_json = json.loads(json_data)
        except json.JSONDecodeError as e:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
            print("âš ï¸ ì¶”ì¶œëœ JSON ì›ë³¸:\n", json_data)
    else:
        print("âŒ JSON êµ¬ì¡°ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ")
        print("âš ï¸ GPT ì¶œë ¥ ì›ë¬¸:\n", gpt_output)

    result_path = 'result,json'

    if not os.path.exists(result_path):
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± + parsed_json ì €ì¥
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(parsed_json, f, ensure_ascii=False, indent=4)
        print("âœ… schema.jsonì´ ì—†ì–´ì„œ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        old_schema = None
    else:
        # íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
        with open(result_path, "r", encoding="utf-8") as f:
            old_schema = json.load(f)
        print("ğŸ“„ ê¸°ì¡´ schema.jsonì„ old_schemaë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        merged_schema = merge_schema_nodes_by_label(old_schema, parsed_json)
        print("old schema")
        print(old_schema)
        print("schema")
        print(parsed_json)
        print("new schema")
        print(merged_schema)
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(merged_schema, f, ensure_ascii=False, indent=4)
        print("âœ… ë³‘í•©ëœ schema ì €ì¥ ì™„ë£Œ: schema.json")

        
    i += 1













