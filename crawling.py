# filename: news_keyword_search.py
import os, json, time, requests
from datetime import datetime
from typing import List, Tuple
from bs4 import BeautifulSoup
from tqdm import tqdm
import numpy as np, faiss
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. OpenAI í´ë¼ì´ì–¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (1) í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì“°ëŠ” ê²Œ ê°€ì¥ ì•ˆì „í•©ë‹ˆë‹¤.
# (2) ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ì§ì ‘ ë¬¸ìì—´ì„ ë„£ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì²˜ëŸ¼â€¦
API_KEY = "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"  # ê·¸ëŒ€ë¡œ!

client = OpenAI(api_key=API_KEY)

EMB_MODEL = "text-embedding-3-small"   # 512â€†orâ€†256 ì°¨ì›
JSON_PATH = "naver_news_top50.json"
IDX_PATH  = "news_index.faiss"
VEC_PATH  = "news_vectors.npy"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION_URL = "https://news.naver.com/main/list.naver"
PARAMS_BASE = {
    "mode": "LS2D",
    "mid":  "sec",
    "sid1": "101",     # ê²½ì œ
    "sid2": "771"      # ê¸°ì—…Â·ê²½ì˜
}
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer":   "https://www.naver.com"
}

def crawl_top50() -> List[dict]:
    """ì˜¤ëŠ˜ ë‚ ì§œ ì„¹ì…˜ í˜ì´ì§€ë¥¼ ëŒë©° ê¸°ì‚¬ 50ê°œë¥¼ ìˆ˜ì§‘í•´ JSONìœ¼ë¡œ ì €ì¥."""
    print("ğŸ“°  ë„¤ì´ë²„ ê¸°ì‚¬ 50ê°œ í¬ë¡¤ë§ ì¤‘â€¦")
    links, page = [], 1
    today = datetime.now().strftime("%Y%m%d")

    # â‘  ë§í¬ ìˆ˜ì§‘
    while len(links) < 50:
        params = PARAMS_BASE | {"page": str(page), "date": today}
        res = requests.get(SECTION_URL, headers=HEADERS,
                           params=params, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        new = []
        for a in soup.select("div.list_body.newsflash_body ul li dl dt a"):
            txt = a.get_text(strip=True)
            if not txt or txt == "ë™ì˜ìƒê¸°ì‚¬":
                continue
            href = a["href"]
            if href.startswith("/"):
                href = "https://news.naver.com" + href
            new.append(href)
        if not new:          # í˜ì´ì§€ê°€ ë” ì—†ìœ¼ë©´ ì¤‘ë‹¨
            break
        for l in new:
            if l not in links:
                links.append(l)
                if len(links) >= 50:
                    break
        page += 1
        time.sleep(0.1)

    # â‘¡ ë³¸ë¬¸ íŒŒì‹±
    def parse(url: str) -> dict:
        r = requests.get(url, headers=HEADERS, timeout=10)
        r.raise_for_status()
        s = BeautifulSoup(r.text, "html.parser")

        title_tag = (s.select_one("#articleTitle")
                     or s.select_one("h2.media_end_head_headline")
                     or s.select_one("h2.media_end_head_title")
                     or s.select_one("h3.end_tit"))
        title = title_tag.get_text(strip=True) if title_tag else ""

        body_tag = (s.select_one("#articleBodyContents")
                    or s.select_one("div#newsct_article._article_body")
                    or s.select_one("div#newsEndContents"))
        content = ""
        if body_tag:
            for t in body_tag.select(
                "script, .end_photo_org, .link_news, span.media_end_linked_more_url"
            ):
                t.decompose()
            content = body_tag.get_text("\n", strip=True)
        return {"title": title, "content": content}

    articles = []
    for i, url in enumerate(links, 1):
        try:
            art = parse(url)
            articles.append(art)
            print(f"[{i:02d}/50] {art['title'][:60]}â€¦")
        except Exception as e:
            print(f"  â†’ ì‹¤íŒ¨({e})")

    # â‘¢ JSON ì €ì¥
    with open(JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ…  í¬ë¡¤ë§ ì™„ë£Œ â†’ {JSON_PATH} ì €ì¥\n")
    return articles

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ì„ë² ë”© & ì¸ë±ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_embedding(text: str) -> List[float]:
    text = text.replace("\n", " ")
    while True:
        try:
            return client.embeddings.create(
                input=[text], model=EMB_MODEL
            ).data[0].embedding
        except Exception as e:
            print("  â³ ì¬ì‹œë„:", e)
            time.sleep(2)

def build_or_load_index() -> Tuple[faiss.IndexFlatIP, List[dict]]:
    """JSONâ†’ë²¡í„°â†’FAISS ì¸ë±ìŠ¤. ìºì‹œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë¡œë“œ."""
    if not os.path.exists(JSON_PATH):
        crawl_top50()                       # íŒŒì¼ ì—†ìœ¼ë©´ ìë™ í¬ë¡¤ë§

    if os.path.exists(IDX_PATH) and os.path.exists(VEC_PATH):
        # â”€ ìºì‹œ ë¡œë“œ â”€
        with open(JSON_PATH, encoding="utf-8") as f:
            articles = json.load(f)
        xb = np.load(VEC_PATH).astype("float32")
        dim = xb.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(xb)
        return index, articles

    # â”€ ìƒˆë¡œ ìƒì„± â”€
    with open(JSON_PATH, encoding="utf-8") as f:
        articles = json.load(f)

    vectors = []
    for art in tqdm(articles, desc="ì„ë² ë”© ìƒì„±"):
        txt = (art["title"] + "\n\n" + art["content"])[:3500]
        vec = np.array(get_embedding(txt), dtype=np.float32)
        vec /= np.linalg.norm(vec) + 1e-10
        vectors.append(vec)

    xb  = np.vstack(vectors).astype("float32")
    dim = xb.shape[1]
    np.save(VEC_PATH, xb)

    index = faiss.IndexFlatIP(dim)
    index.add(xb)
    faiss.write_index(index, IDX_PATH)
    return index, articles

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. í‚¤ì›Œë“œ ê²€ìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_by_keywords(keywords: List[str], top_k: int = 1) -> List[dict]:
    query = " ".join(keywords)
    q_vec = np.array(get_embedding(query), dtype=np.float32)
    q_vec /= np.linalg.norm(q_vec) + 1e-10

    index, articles = build_or_load_index()
    D, I = index.search(q_vec[None, :], top_k)

    results = []
    for score, idx in zip(D[0], I[0]):
        art = articles[int(idx)].copy()
        art["score"] = float(score)
        results.append(art)
    return results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    kws = input("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„): ").split()
    top = search_by_keywords(kws, top_k=5)

    print("\nğŸ”  í‚¤ì›Œë“œ ì—°ê´€ ê¸°ì‚¬ Top-5")
    for i, art in enumerate(top, 5):
        print(f"[{i}] ({art['score']:.3f}) {art['title']}")
    # ê²°ê³¼ ì €ì¥
    with open("keyword_results.json", "w", encoding="utf-8") as f:
        json.dump(top, f, ensure_ascii=False, indent=2)
    print("\nâœ…  ê²°ê³¼ â†’ keyword_results.json ì €ì¥")
