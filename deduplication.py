import os
import json
import openai
from util import parse_json
from dotenv import load_dotenv
from typing import Dict, List, Set, Tuple
from collections import Counter, defaultdict
import re

def analyze_graph_integrity(data: Dict) -> Dict:
    """ê·¸ëž˜í”„ ë¬´ê²°ì„± ë¶„ì„ (PDF ì œì•ˆì‚¬í•­)"""
    nodes = data.get('nodes', [])
    relations = data.get('relations', [])
    
    # ë…¸ë“œ ì´ë¦„ ì§‘í•©
    node_names = {node['name'] for node in nodes}
    
    # ê´€ê³„ì—ì„œ ì°¸ì¡°ë˜ëŠ” ë…¸ë“œë“¤
    referenced_nodes = set()
    for rel in relations:
        referenced_nodes.add(rel['start_node'])
        referenced_nodes.add(rel['end_node'])
    
    # ë¬´ê²°ì„± ë¶„ì„
    orphaned_nodes = node_names - referenced_nodes  # ê´€ê³„ê°€ ì—†ëŠ” ê³ ë¦½ëœ ë…¸ë“œ
    missing_nodes = referenced_nodes - node_names   # ê´€ê³„ì—ì„œ ì°¸ì¡°ë˜ì§€ë§Œ ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ë…¸ë“œ
    
    # ë…¸ë“œë³„ ì—°ê²°ë„ ë¶„ì„
    node_connections = defaultdict(int)
    for rel in relations:
        node_connections[rel['start_node']] += 1
        node_connections[rel['end_node']] += 1
    
    # ì¤‘ì•™ì„±ì´ ë†’ì€ ë…¸ë“œë“¤ (ì—°ê²°ì´ ë§Žì€ ë…¸ë“œ)
    central_nodes = sorted(node_connections.items(), key=lambda x: x[1], reverse=True)[:10]
    
    # ê´€ê³„ íƒ€ìž… ë¶„í¬
    relation_types = Counter(rel['relationship'] for rel in relations)
    
    # ì¤‘ë³µ ê´€ê³„ ê²€ì‚¬
    relation_keys = [(rel['start_node'], rel['relationship'], rel['end_node']) for rel in relations]
    duplicate_relations = [k for k, count in Counter(relation_keys).items() if count > 1]
    
    return {
        "total_nodes": len(nodes),
        "total_relations": len(relations),
        "orphaned_nodes": list(orphaned_nodes),
        "missing_nodes": list(missing_nodes),
        "central_nodes": central_nodes,
        "relation_types": dict(relation_types),
        "duplicate_relations": duplicate_relations,
        "connectivity_score": len(referenced_nodes) / len(nodes) if nodes else 0
    }

def repair_missing_entities(data: Dict, api_key: str) -> Dict:
    """ëˆ„ë½ëœ ì—”í‹°í‹° ë³µêµ¬ ì‹œë„ (PDF ì œì•ˆì‚¬í•­)"""
    client = openai.OpenAI(api_key=api_key)
    
    integrity_analysis = analyze_graph_integrity(data)
    missing_nodes = integrity_analysis['missing_nodes']
    
    if not missing_nodes:
        return data
    
    print(f"ðŸ”§ ëˆ„ë½ëœ ì—”í‹°í‹° ë³µêµ¬ ì‹œë„: {len(missing_nodes)}ê°œ")
    
    # ëˆ„ë½ëœ ì—”í‹°í‹°ì— ëŒ€í•œ ì •ë³´ ì¶”ë¡ 
    existing_nodes = data.get('nodes', [])
    relations = data.get('relations', [])
    
    # ëˆ„ë½ëœ ê° ì—”í‹°í‹°ì— ëŒ€í•´ ê´€ê³„ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    for missing_node in missing_nodes:
        related_relations = []
        for rel in relations:
            if rel['start_node'] == missing_node or rel['end_node'] == missing_node:
                related_relations.append(rel)
        
        if related_relations:
            # ê´€ê³„ ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° ì—”í‹°í‹° íƒ€ìž… ì¶”ë¡ 
            context_prompt = f"""ëˆ„ë½ëœ ì—”í‹°í‹° '{missing_node}'ì˜ íƒ€ìž…ì„ ë‹¤ìŒ ê´€ê³„ë“¤ë¡œë¶€í„° ì¶”ë¡ í•˜ì„¸ìš”:

ê´€ê³„ ì •ë³´:
{json.dumps(related_relations, ensure_ascii=False, indent=2)}

ê¸°ì¡´ ë…¸ë“œ ì˜ˆì‹œ:
{json.dumps(existing_nodes[:5], ensure_ascii=False, indent=2)}

ë‹¤ìŒ ì¤‘ ê°€ìž¥ ì ì ˆí•œ ë¼ë²¨ì„ ì„ íƒí•˜ê³  ê¸°ë³¸ ì†ì„±ì„ ì¶”ë¡ í•˜ì„¸ìš”:
PERSON, COMPANY, ORGANIZATION, LOCATION, EVENT, PRODUCT, TECHNOLOGY, PROJECT

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "label": "ì¶”ë¡ ëœ_ë¼ë²¨",
    "name": "{missing_node}",
    "properties": {{
        "inferred": true,
        "confidence": "high|medium|low",
        "reasoning": "ì¶”ë¡  ê·¼ê±°"
    }}
}}"""

            try:
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": "ì—”í‹°í‹° íƒ€ìž… ì¶”ë¡  ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                        {"role": "user", "content": context_prompt}
                    ],
                    temperature=0.1
                )
                
                inferred_entity = parse_json(response.choices[0].message.content)
                data['nodes'].append(inferred_entity)
                print(f"   ë³µêµ¬ë¨: {missing_node} â†’ {inferred_entity['label']}")
                
            except Exception as e:
                print(f"   ë³µêµ¬ ì‹¤íŒ¨: {missing_node} - {e}")
    
    return data

def enhanced_similarity_check(nodes: List[Dict]) -> List[Tuple[int, int, float]]:
    """í–¥ìƒëœ ë…¸ë“œ ìœ ì‚¬ë„ ê²€ì‚¬"""
    from difflib import SequenceMatcher
    
    similar_pairs = []
    
    for i, node1 in enumerate(nodes):
        for j, node2 in enumerate(nodes[i+1:], i+1):
            # ê°™ì€ ë¼ë²¨ì˜ ë…¸ë“œë“¤ë§Œ ë¹„êµ
            if node1['label'] != node2['label']:
                continue
            
            name1 = node1['name'].lower()
            name2 = node2['name'].lower()
            
            # ë¬¸ìžì—´ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = SequenceMatcher(None, name1, name2).ratio()
            
            # ë†’ì€ ìœ ì‚¬ë„ ë˜ëŠ” í¬í•¨ ê´€ê³„
            if similarity > 0.8 or name1 in name2 or name2 in name1:
                similar_pairs.append((i, j, similarity))
    
    return similar_pairs

def smart_deduplication(data_path: str):
    """ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±° (PDF ì œì•ˆì‚¬í•­ ë°˜ì˜)"""
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    with open(data_path, "r", encoding="utf-8") as f:
        result_json = json.load(f)

    # ë°±ì—… ì €ìž¥
    root, ext = os.path.splitext(data_path)
    backup_path = f"{root}_backup{ext}"
    with open(backup_path, "w", encoding="utf-8") as f:
        json.dump(result_json, f, ensure_ascii=False, indent=4)

    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")

    # 1ë‹¨ê³„: ê·¸ëž˜í”„ ë¬´ê²°ì„± ë¶„ì„
    print("ðŸ” 1ë‹¨ê³„: ê·¸ëž˜í”„ ë¬´ê²°ì„± ë¶„ì„")
    integrity_report = analyze_graph_integrity(result_json)
    
    print(f"   ë…¸ë“œ: {integrity_report['total_nodes']}ê°œ")
    print(f"   ê´€ê³„: {integrity_report['total_relations']}ê°œ")
    print(f"   ê³ ë¦½ëœ ë…¸ë“œ: {len(integrity_report['orphaned_nodes'])}ê°œ")
    print(f"   ëˆ„ë½ëœ ë…¸ë“œ: {len(integrity_report['missing_nodes'])}ê°œ")
    print(f"   ì—°ê²°ì„± ì ìˆ˜: {integrity_report['connectivity_score']:.2f}")
    
    if integrity_report['missing_nodes']:
        print(f"   ëˆ„ë½ëœ ë…¸ë“œ ëª©ë¡: {integrity_report['missing_nodes'][:10]}")

    # 2ë‹¨ê³„: ëˆ„ë½ëœ ì—”í‹°í‹° ë³µêµ¬
    print("ðŸ”§ 2ë‹¨ê³„: ëˆ„ë½ëœ ì—”í‹°í‹° ë³µêµ¬")
    result_json = repair_missing_entities(result_json, api_key)

    # 3ë‹¨ê³„: ìœ ì‚¬ ë…¸ë“œ ê²€ì‚¬ ë° ë³‘í•©
    print("ðŸ” 3ë‹¨ê³„: ìœ ì‚¬ ë…¸ë“œ ê²€ì‚¬")
    similar_pairs = enhanced_similarity_check(result_json['nodes'])
    
    if similar_pairs:
        print(f"   ìœ ì‚¬í•œ ë…¸ë“œ ìŒ: {len(similar_pairs)}ê°œ")
        for i, j, sim in similar_pairs[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
            node1 = result_json['nodes'][i]
            node2 = result_json['nodes'][j]
            print(f"   - {node1['name']} â†” {node2['name']} (ìœ ì‚¬ë„: {sim:.2f})")

    # 4ë‹¨ê³„: LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±°
    print("ðŸ¤– 4ë‹¨ê³„: LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±°")
    client = openai.OpenAI(api_key=api_key)

    # ì¤‘ë³µ ì œê±° í”„ë¡¬í”„íŠ¸ ê°œì„ 
    enhanced_prompt = f"""ë‹¤ìŒ ì§€ì‹ ê·¸ëž˜í”„ì˜ ë…¸ë“œì™€ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì¤‘ë³µ ë° í’ˆì§ˆ ì´ìŠˆë¥¼ í•´ê²°í•˜ì„¸ìš”.

### í˜„ìž¬ ë°ì´í„°:
{json.dumps(result_json, ensure_ascii=False, indent=2)[:8000]}  # í† í° ì œí•œ

### ë¬´ê²°ì„± ë¶„ì„ ê²°ê³¼:
- ì´ ë…¸ë“œ: {integrity_report['total_nodes']}ê°œ
- ì´ ê´€ê³„: {integrity_report['total_relations']}ê°œ  
- ê³ ë¦½ëœ ë…¸ë“œ: {len(integrity_report['orphaned_nodes'])}ê°œ
- ì¤‘ë³µ ê´€ê³„: {len(integrity_report['duplicate_relations'])}ê°œ

### ê°œì„  ì§€ì¹¨:
1. **ë˜‘ë˜‘í•œ ì¤‘ë³µ ì œê±°**: 
   - ì˜ë¯¸ìƒ ë™ì¼í•œ ì—”í‹°í‹° ë³‘í•© (ì˜ˆ: "êµ¬ê¸€" + "Google" â†’ "êµ¬ê¸€")
   - ì†ì„± ì •ë³´ ì†ì‹¤ ì—†ì´ ë³‘í•©
   - ë³„ì¹­ ì •ë³´ëŠ” aliases ë°°ì—´ì— ë³´ì¡´

2. **ê´€ê³„ í’ˆì§ˆ í–¥ìƒ**:
   - ì¤‘ë³µ ê´€ê³„ ì œê±°
   - ë¶ˆí•„ìš”í•œ ê´€ê³„ ì •ë¦¬ (ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ê´€ê³„)
   - ê´€ê³„ ë°©í–¥ ì¼ê´€ì„± í™•ì¸

3. **ë…¸ë“œ í’ˆì§ˆ í–¥ìƒ**:
   - ë„ˆë¬´ ì¼ë°˜ì ì¸ ë…¸ë“œ ì œê±° ("ê¸°ìˆ ", "ì‹œìž¥" ë“±)
   - êµ¬ì²´ì  ë…¸ë“œê°€ ìžˆìœ¼ë©´ ì¼ë°˜ì  ë…¸ë“œ ì œê±° (ì˜ˆ: "ì•„ì‹œì•„" vs "í•œêµ­", "ì¼ë³¸")
   - ì†ì„± ì •ë³´ í’ë¶€í™”

4. **ë§¥ë½ ì¼ê´€ì„±**:
   - í•œêµ­ì–´ ë‰´ìŠ¤ ë„ë©”ì¸ì— ì í•©í•œ ì—”í‹°í‹°ë§Œ ìœ ì§€
   - ë¬¸ë§¥ìƒ ì¤‘ìš”í•˜ì§€ ì•Šì€ ì—”í‹°í‹° ì œê±°

### ì¶œë ¥:
ì •ì œëœ ë…¸ë“œì™€ ê´€ê³„ë§Œ í¬í•¨í•˜ëŠ” JSON"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ì§€ì‹ ê·¸ëž˜í”„ í’ˆì§ˆ ê°œì„  ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                {"role": "user", "content": enhanced_prompt}
            ],
            temperature=0.1
        )

        # ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ
        gpt_output = response.choices[0].message.content
        
        # JSON ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ
        json_match = re.search(r'```json\s*(.*?)\s*```', gpt_output, re.DOTALL)
        if json_match:
            json_content = json_match.group(1)
        else:
            json_content = gpt_output
        
        cleaned_json = parse_json(json_content)

        # 5ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ë° í†µê³„
        print("ðŸ“Š 5ë‹¨ê³„: ê°œì„  ê²°ê³¼ ê²€ì¦")
        
        original_nodes = len(result_json['nodes'])
        original_relations = len(result_json['relations'])
        cleaned_nodes = len(cleaned_json.get('nodes', []))
        cleaned_relations = len(cleaned_json.get('relations', []))
        
        print(f"   ë…¸ë“œ: {original_nodes} â†’ {cleaned_nodes} ({cleaned_nodes - original_nodes:+d})")
        print(f"   ê´€ê³„: {original_relations} â†’ {cleaned_relations} ({cleaned_relations - original_relations:+d})")
        
        # ìµœì¢… ë¬´ê²°ì„± ê²€ì‚¬
        final_integrity = analyze_graph_integrity(cleaned_json)
        print(f"   ìµœì¢… ì—°ê²°ì„±: {final_integrity['connectivity_score']:.2f}")
        print(f"   ìµœì¢… ê³ ë¦½ ë…¸ë“œ: {len(final_integrity['orphaned_nodes'])}ê°œ")

        # ì¤‘ìš”í•œ ê°œì„ ì‚¬í•­ì´ ìžˆëŠ” ê²½ìš°ë§Œ ì ìš©
        if (final_integrity['connectivity_score'] >= integrity_report['connectivity_score'] * 0.9 and 
            len(final_integrity['missing_nodes']) <= len(integrity_report['missing_nodes'])):
            
            with open(data_path, "w", encoding="utf-8") as f:
                json.dump(cleaned_json, f, ensure_ascii=False, indent=4)
            print("âœ… ê°œì„ ëœ ë°ì´í„° ì €ìž¥ ì™„ë£Œ")
        else:
            print("âš ï¸ í’ˆì§ˆ ì €í•˜ë¡œ ì›ë³¸ ìœ ì§€")
            
    except Exception as e:
        print(f"LLM ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")

def validate_graph_quality(data_path: str) -> Dict:
    """ê·¸ëž˜í”„ í’ˆì§ˆ ê²€ì¦ (PDF ì œì•ˆì‚¬í•­)"""
    with open(data_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    nodes = data.get('nodes', [])
    relations = data.get('relations', [])
    
    quality_metrics = {
        "node_coverage": 0,      # ê´€ê³„ì— ì°¸ì—¬í•˜ëŠ” ë…¸ë“œ ë¹„ìœ¨
        "relation_density": 0,   # ê°€ëŠ¥í•œ ê´€ê³„ ëŒ€ë¹„ ì‹¤ì œ ê´€ê³„ ë¹„ìœ¨  
        "avg_connections": 0,    # ë…¸ë“œë‹¹ í‰ê·  ì—°ê²° ìˆ˜
        "isolated_nodes": 0,     # ê³ ë¦½ëœ ë…¸ë“œ ìˆ˜
        "relation_diversity": 0, # ê´€ê³„ íƒ€ìž… ë‹¤ì–‘ì„±
        "has_aliases": 0,        # ë³„ì¹­ ì •ë³´ê°€ ìžˆëŠ” ë…¸ë“œ ìˆ˜
        "property_richness": 0   # ì†ì„±ì´ í’ë¶€í•œ ë…¸ë“œ ë¹„ìœ¨
    }
    
    if not nodes:
        return quality_metrics
    
    # ë…¸ë“œ ì»¤ë²„ë¦¬ì§€
    node_names = {node['name'] for node in nodes}
    referenced_nodes = set()
    for rel in relations:
        referenced_nodes.add(rel['start_node'])
        referenced_nodes.add(rel['end_node'])
    
    quality_metrics["node_coverage"] = len(referenced_nodes) / len(nodes)
    quality_metrics["isolated_nodes"] = len(nodes) - len(referenced_nodes)
    
    # ê´€ê³„ ë°€ë„
    max_possible_relations = len(nodes) * (len(nodes) - 1)  # ë°©í–¥ì„± ìžˆëŠ” ê´€ê³„
    if max_possible_relations > 0:
        quality_metrics["relation_density"] = len(relations) / max_possible_relations
    
    # í‰ê·  ì—°ê²° ìˆ˜
    if nodes:
        quality_metrics["avg_connections"] = (len(relations) * 2) / len(nodes)
    
    # ê´€ê³„ ë‹¤ì–‘ì„±
    relation_types = set(rel['relationship'] for rel in relations)
    quality_metrics["relation_diversity"] = len(relation_types)
    
    # ë³„ì¹­ ì •ë³´
    quality_metrics["has_aliases"] = sum(1 for node in nodes 
                                       if 'aliases' in node.get('properties', {}))
    
    # ì†ì„± í’ë¶€ë„ (ì†ì„±ì´ 2ê°œ ì´ìƒì¸ ë…¸ë“œ)
    quality_metrics["property_richness"] = sum(1 for node in nodes 
                                             if len(node.get('properties', {})) >= 2) / len(nodes)
    
    return quality_metrics

def main_enhanced_deduplication(data_path: str):
    """ë©”ì¸ í–¥ìƒëœ ì¤‘ë³µ ì œê±° í•¨ìˆ˜"""
    print("ðŸš€ í–¥ìƒëœ ì§€ì‹ ê·¸ëž˜í”„ í’ˆì§ˆ ê°œì„  ì‹œìž‘")
    
    # ì‚¬ì „ í’ˆì§ˆ ì¸¡ì •
    print("ðŸ“Š ì‚¬ì „ í’ˆì§ˆ ì¸¡ì •")
    pre_quality = validate_graph_quality(data_path)
    print(f"   ë…¸ë“œ ì»¤ë²„ë¦¬ì§€: {pre_quality['node_coverage']:.2f}")
    print(f"   ê´€ê³„ ë‹¤ì–‘ì„±: {pre_quality['relation_diversity']}ê°œ íƒ€ìž…")
    print(f"   ê³ ë¦½ ë…¸ë“œ: {pre_quality['isolated_nodes']}ê°œ")
    print(f"   ì†ì„± í’ë¶€ë„: {pre_quality['property_richness']:.2f}")
    
    # ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±° ìˆ˜í–‰
    smart_deduplication(data_path)
    
    # ì‚¬í›„ í’ˆì§ˆ ì¸¡ì •
    print("ðŸ“Š ì‚¬í›„ í’ˆì§ˆ ì¸¡ì •")
    post_quality = validate_graph_quality(data_path)
    print(f"   ë…¸ë“œ ì»¤ë²„ë¦¬ì§€: {post_quality['node_coverage']:.2f} ({post_quality['node_coverage'] - pre_quality['node_coverage']:+.2f})")
    print(f"   ê´€ê³„ ë‹¤ì–‘ì„±: {post_quality['relation_diversity']}ê°œ íƒ€ìž… ({post_quality['relation_diversity'] - pre_quality['relation_diversity']:+d})")
    print(f"   ê³ ë¦½ ë…¸ë“œ: {post_quality['isolated_nodes']}ê°œ ({post_quality['isolated_nodes'] - pre_quality['isolated_nodes']:+d})")
    print(f"   ì†ì„± í’ë¶€ë„: {post_quality['property_richness']:.2f} ({post_quality['property_richness'] - pre_quality['property_richness']:+.2f})")
    
    print("âœ… í–¥ìƒëœ ì§€ì‹ ê·¸ëž˜í”„ í’ˆì§ˆ ê°œì„  ì™„ë£Œ")

# ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ëž˜í¼
def deduplicate(data_path: str):
    """ê¸°ì¡´ ì¸í„°íŽ˜ì´ìŠ¤ í˜¸í™˜ì„± ìœ ì§€"""
    main_enhanced_deduplication(data_path)

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        main_enhanced_deduplication(sys.argv[1])
    else:
        main_enhanced_deduplication("output/result/result.json")