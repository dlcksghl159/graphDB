# filename: schema_extract_mp.py
import os, json, glob
import multiprocessing as mp
from dotenv import load_dotenv
import openai

from util import merge_json, parse_json   # ê¸°ì¡´ util ê·¸ëŒ€ë¡œ ì‚¬ìš©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì „ì—­ ì„¤ì • â”€ í”„ë¡œì„¸ìŠ¤ë“¤ì—ì„œ ê³µìœ  (ì½ê¸° ì „ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_ROOT   = os.getenv("OUTPUT_ROOT", "output")
SCHEMA_DIR    = os.path.join(OUTPUT_ROOT, "schema")
CHUNKS_DIR    = os.path.join(OUTPUT_ROOT, "chunked_document")
os.makedirs(SCHEMA_DIR, exist_ok=True)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì „ì—­ ìƒìˆ˜)
SYSTEM_MSG = (
    "ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œìš© ì§€ì‹ ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ(ì—”í‹°í‹°/ê´€ê³„ íƒ€ì…)ë¥¼ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤. "
    "ë°˜ë“œì‹œ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì›Œì»¤ í•¨ìˆ˜ â€“ í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _process_chunk(args: tuple[int, str, str]) -> dict:
    """
    íŒŒë¼ë¯¸í„°
      idx      : chunk ì¸ë±ìŠ¤
      purpose  : ì‚¬ìš©ì ì…ë ¥ ëª©ì 
      system   : ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    ë°˜í™˜ê°’
      parsed_json (dict) â€“ ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ
    """
    idx, purpose, system = args

    # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ í™˜ê²½-ë³€ìˆ˜, OpenAI client ì´ˆê¸°í™”
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    client  = openai.OpenAI(api_key=api_key)

    fname = f"{CHUNKS_DIR}/chunked_output_{idx}.txt"
    with open(fname, "r", encoding="utf-8") as f:
        content = f.read()

    prompt = f"""
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **'{purpose}'** ëª©ì ì˜ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì§€ì‹ ê·¸ë˜í”„ì˜ **ìŠ¤í‚¤ë§ˆ(Schema)**ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ì˜í•˜ì„¸ìš”.

    ### ì‘ì„± ì§€ì¹¨:
    1. `nodes`ì—ëŠ” **ì§€ì‹ ê·¸ë˜í”„ì— ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•  ì£¼ìš” ê°œì²´ ìœ í˜•**(Node)ì„ ì •ì˜í•©ë‹ˆë‹¤.
    2. ê° NodeëŠ” `label`, `name`, `properties` í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•˜ë©°, `properties`ì˜ ê° í•­ëª© ê°’ì€ `"string"`, `"int"` ë“± **ë°ì´í„° íƒ€ì… ë¬¸ìì—´**ë¡œ ê¸°ì…í•©ë‹ˆë‹¤.
    3. `relations`ì—ëŠ” ê°œì²´ ê°„ì˜ **ê´€ê³„ ìœ í˜•(Relation)**ì„ ì •ì˜í•˜ë©°, `start_node`, `end_node`ì—ëŠ” **ë…¸ë“œì˜ ë¼ë²¨(label)**ë§Œ ê¸°ì…í•©ë‹ˆë‹¤.
    4. **ë¶ˆí•„ìš”í•˜ê²Œ ë³µì¡í•œ ì†ì„±ì€ ìƒëµ**í•˜ê³ , **RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì— ê¼­ í•„ìš”í•œ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ í¬í•¨**í•˜ì„¸ìš”.
    5. ìµœì¢… ì¶œë ¥ì€ **ì˜¬ë°”ë¥¸ JSON í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª… ì—†ì´ **JSONë§Œ ì¶œë ¥**í•˜ì„¸ìš”.

    ### ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
    {{
        "nodes": [
            {{"label": "NODE_LABEL", "name": "String", "properties": {{"key": "ë°ì´í„°íƒ€ì…"}}}}
        ],
        "relations": [
            {{"start_node": "NodeLabel", "relationship": "RELATION_NAME", "end_node": "NodeLabel", "properties": {{"key": "ë°ì´í„°íƒ€ì…"}}}}
        ]
    }}

    ### í…ìŠ¤íŠ¸:
    {content}
    """

    resp = client.chat.completions.create(
        model="gpt-4.1",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]
    )

    parsed = parse_json(resp.choices[0].message.content)

    # ê°œë³„ ê²°ê³¼ ì €ì¥
    out_path = os.path.join(SCHEMA_DIR, f"schema_{idx}.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(parsed, f, ensure_ascii=False, indent=2)

    print(f"[{idx}] schema ì¶”ì¶œ ì™„ë£Œ")
    return parsed


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë©”ì¸ â€“ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ â†’ ë³‘ë ¬ ì‹¤í–‰ â†’ ë¨¸ì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_mp(max_workers: int = 15, purpose = "ê¸°ì—… íŒë§¤"):
    # ì²˜ë¦¬í•  chunk íŒŒì¼ ì¸ë±ìŠ¤ ê³„ì‚°
    files = sorted(glob.glob(os.path.join(CHUNKS_DIR, "chunked_output_*.txt")))
    if not files:
        print("âš ï¸  chunked_document í´ë”ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    indices = [int(os.path.splitext(os.path.basename(f))[0].split("_")[-1]) for f in files]

    # Pool ì‹¤í–‰
    print(f"ğŸš€ ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ (workers={max_workers}, ì´ {len(indices)}ê°œ)â€¦")
    with mp.Pool(processes=max_workers) as pool:
        all_schemas = pool.map(_process_chunk, [(i, purpose, SYSTEM_MSG) for i in indices])

    # ----------- ìµœì¢… ë¨¸ì§€ -----------
    merged_path = os.path.join(SCHEMA_DIR, "schema.json")
    merged = {}
    for sc in all_schemas:                # ìˆœì„œ ìƒê´€ì—†ì´ head-tail ë¨¸ì§€
        merged = merge_json(merged, sc, node_key=("label",))

    with open(merged_path, "w", encoding="utf-8") as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)

    print(f"ğŸ‰ ëª¨ë“  ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ë° ë³‘í•© ì™„ë£Œ â†’ {merged_path}")

def main(purpose="ì¢…í•© ë‰´ìŠ¤ ë¶„ì„"):
    extract_mp(max_workers=10, purpose=purpose)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()
    # CPUê°€ ë§ì•„ë„ API rate-limitì„ ê³ ë ¤í•´ 4~6ê°œ ì •ë„ê°€ ì•ˆì „
    