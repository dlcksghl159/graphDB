import os
import json
import openai
import re
from tqdm import tqdm
from util import merge_json, parse_json
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Set, Tuple
import hashlib

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
PURPOSE = os.getenv("PURPOSE", "ë¬¸ì„œ ë¶„ì„")

# í•˜ë“œì½”ë”©ëœ ë™ì˜ì–´ ì‚¬ì „ê³¼ íŒ¨í„´ ì œê±°
# ëŒ€ì‹  ë™ì ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” ì¼ë°˜ì ì¸ ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©

class GeneralEntityTracker:
    """ë„ë©”ì¸ ë…ë¦½ì ì¸ ì—”í‹°í‹° ì¶”ì ê¸°"""
    def __init__(self):
        self.entities = {}  # fingerprint -> entity
        self.name_to_fingerprints = {}  # normalized_name -> set of fingerprints
        self.aliases = {}  # fingerprint -> set of aliases
        self.entity_contexts = {}  # fingerprint -> list of contexts
        
    def normalize_entity_name(self, name: str, entity_type: str) -> str:
        """ê¸°ë³¸ì ì¸ ì •ê·œí™”ë§Œ ìˆ˜í–‰ (ë„ë©”ì¸ íŠ¹í™” ë¡œì§ ì œê±°)"""
        # ê¸°ë³¸ ì •ê·œí™”: ê³µë°± ì •ë¦¬, ëŒ€ì†Œë¬¸ì í†µì¼
        normalized = name.strip()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ê¸°ë³¸ì ì¸ ê²ƒë§Œ)
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = re.sub(r'[^\w\s\-\.]', '', normalized)
        
        return normalized
    
    def generate_entity_fingerprint(self, entity: Dict) -> str:
        """ì—”í‹°í‹°ì˜ ê³ ìœ  ì§€ë¬¸ ìƒì„±"""
        normalized_name = self.normalize_entity_name(entity['name'], entity['label'])
        # íƒ€ì…ê³¼ ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ì§€ë¬¸ ìƒì„±
        fingerprint_str = f"{entity['label']}:{normalized_name}".lower()
        return hashlib.md5(fingerprint_str.encode()).hexdigest()
    
    def add_entity(self, entity: Dict, context: str = "") -> Tuple[bool, str]:
        """ì—”í‹°í‹° ì¶”ê°€ ë° ì¤‘ë³µ ê²€ì‚¬"""
        fingerprint = self.generate_entity_fingerprint(entity)
        
        if fingerprint in self.entities:
            # ê¸°ì¡´ ì—”í‹°í‹° ì—…ë°ì´íŠ¸
            existing = self.entities[fingerprint]
            
            # ì†ì„± ë³‘í•©
            if 'properties' not in existing:
                existing['properties'] = {}
            for key, value in entity.get('properties', {}).items():
                if key not in existing['properties']:
                    existing['properties'][key] = value
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if fingerprint in self.entity_contexts:
                self.entity_contexts[fingerprint].append(context[:200])
            
            # ë³„ì¹­ ì¶”ê°€
            if fingerprint not in self.aliases:
                self.aliases[fingerprint] = set()
            self.aliases[fingerprint].add(entity['name'])
            
            return False, fingerprint
        
        # ìƒˆ ì—”í‹°í‹° ì¶”ê°€
        normalized_name = self.normalize_entity_name(entity['name'], entity['label'])
        entity['name'] = normalized_name
        self.entities[fingerprint] = entity
        
        # ë³„ì¹­ ì´ˆê¸°í™”
        self.aliases[fingerprint] = {entity['name']}
        
        # ì»¨í…ìŠ¤íŠ¸ ì €ì¥
        self.entity_contexts[fingerprint] = [context[:200]]
        
        # ì´ë¦„ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        name_lower = normalized_name.lower()
        if name_lower not in self.name_to_fingerprints:
            self.name_to_fingerprints[name_lower] = set()
        self.name_to_fingerprints[name_lower].add(fingerprint)
        
        return True, fingerprint
    
    def find_similar_entities(self, entity: Dict, threshold: float = 0.8) -> List[str]:
        """ìœ ì‚¬í•œ ì—”í‹°í‹° ì°¾ê¸° (ë„ë©”ì¸ ë…ë¦½ì )"""
        from difflib import SequenceMatcher
        
        similar_fingerprints = []
        entity_name_lower = entity['name'].lower()
        
        for existing_fp, existing_entity in self.entities.items():
            # ê°™ì€ íƒ€ì…ì˜ ì—”í‹°í‹°ë§Œ ë¹„êµ
            if existing_entity['label'] != entity['label']:
                continue
            
            existing_name_lower = existing_entity['name'].lower()
            
            # ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = SequenceMatcher(None, entity_name_lower, existing_name_lower).ratio()
            
            if similarity > threshold:
                similar_fingerprints.append((existing_fp, similarity))
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_fingerprints.sort(key=lambda x: x[1], reverse=True)
        return [fp for fp, _ in similar_fingerprints]
    
    def get_all_entities(self) -> List[Dict]:
        """ëª¨ë“  ì—”í‹°í‹° ë°˜í™˜ (ë³„ì¹­ ì •ë³´ í¬í•¨)"""
        result = []
        for fingerprint, entity in self.entities.items():
            entity_copy = entity.copy()
            if 'properties' not in entity_copy:
                entity_copy['properties'] = {}
            
            # ë³„ì¹­ ì¶”ê°€
            if fingerprint in self.aliases and len(self.aliases[fingerprint]) > 1:
                entity_copy['properties']['aliases'] = list(self.aliases[fingerprint])
            
            # ì»¨í…ìŠ¤íŠ¸ ìˆ˜ ì¶”ê°€ (ë””ë²„ê¹…ìš©)
            if fingerprint in self.entity_contexts:
                entity_copy['properties']['mention_count'] = len(self.entity_contexts[fingerprint])
            
            result.append(entity_copy)
        return result

# ì „ì—­ ì¶”ì ê¸°
entity_tracker = GeneralEntityTracker()

def extract_entities_with_context(text: str, schema: Dict) -> List[Dict]:
    """í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ìœ¼ë¡œ ì—”í‹°í‹° ì¶”ì¶œ (ë„ë©”ì¸ ë…ë¦½ì )"""
    entities = []
    
    # ìŠ¤í‚¤ë§ˆì—ì„œ ë…¸ë“œ íƒ€ì… ì¶”ì¶œ
    node_types = [node['label'] for node in schema.get('nodes', [])]
    
    # ê¸°ë³¸ íŒ¨í„´: ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ (ê³ ìœ ëª…ì‚¬ ê°€ëŠ¥ì„±)
    proper_noun_pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
    
    # ìˆ«ì í¬í•¨ íŒ¨í„´ (ë²„ì „, ëª¨ë¸ëª… ë“±)
    alphanumeric_pattern = r'\b[A-Z][A-Za-z0-9\-\.]+\b'
    
    # ì¸ìš©ë¶€í˜¸ ë‚´ í…ìŠ¤íŠ¸
    quoted_pattern = r'["\'](.*?)["\']'
    
    # ê´„í˜¸ ë‚´ ì•½ì–´
    acronym_pattern = r'\(([A-Z]{2,})\)'
    
    # ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ
    candidates = set()
    
    for pattern in [proper_noun_pattern, alphanumeric_pattern]:
        matches = re.finditer(pattern, text)
        for match in matches:
            candidate = match.group().strip()
            if len(candidate) > 2:  # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸
                candidates.add(candidate)
    
    # ì¸ìš©ë¶€í˜¸ ë‚´ í…ìŠ¤íŠ¸
    quoted_matches = re.finditer(quoted_pattern, text)
    for match in quoted_matches:
        candidate = match.group(1).strip()
        if len(candidate) > 2 and len(candidate) < 100:
            candidates.add(candidate)
    
    # ì•½ì–´
    acronym_matches = re.finditer(acronym_pattern, text)
    for match in acronym_matches:
        candidates.add(match.group(1))
    
    # í›„ë³´ë¥¼ ì—”í‹°í‹°ë¡œ ë³€í™˜ (íƒ€ì…ì€ ë‚˜ì¤‘ì— LLMì´ ê²°ì •)
    for candidate in candidates:
        entities.append({
            "name": candidate,
            "label": "ENTITY",  # ê¸°ë³¸ ë ˆì´ë¸”
            "properties": {}
        })
    
    return entities

def process_file_general(idx, filename, chunks_dir, result_dir, schema_json, api_key, system_msg, purpose):
    """ì¼ë°˜í™”ëœ íŒŒì¼ë³„ ë…¸ë“œ ì¶”ì¶œ"""
    client = openai.OpenAI(api_key=api_key)
    filename_path = os.path.join(chunks_dir, filename)
    
    if not os.path.exists(filename_path):
        print(f"íŒŒì¼ ì—†ìŒ: {filename_path} â†’ ê±´ë„ˆëœ€")
        return None
    
    with open(filename_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # íŒ¨í„´ ê¸°ë°˜ í›„ë³´ ì¶”ì¶œ
    pattern_candidates = extract_entities_with_context(content, schema_json)
    
    # ê¸°ì¡´ ì—”í‹°í‹° ëª©ë¡ (ì¼ê´€ì„± ìœ ì§€ìš©)
    existing_entities = entity_tracker.get_all_entities()
    existing_names_str = ""
    if existing_entities:
        sample_entities = existing_entities[:30]  # ìƒ˜í”Œë§Œ í‘œì‹œ
        existing_names_str = f"""
### ì´ë¯¸ ì¶”ì¶œëœ ì—”í‹°í‹° ì˜ˆì‹œ (ì¼ê´€ì„± ì°¸ê³ ):
{json.dumps(sample_entities, ensure_ascii=False, indent=2)}
"""
    
    prompt = f"""ë‹¹ì‹ ì€ **'{purpose}'** ëª©ì ì˜ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ìœ„í•œ ì—”í‹°í‹°(ë…¸ë“œ)ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

### ìŠ¤í‚¤ë§ˆ:
{json.dumps(schema_json, ensure_ascii=False, indent=2)}

### ì§€ì¹¨:
1. **ë„ë©”ì¸ ë…ë¦½ì  ì¶”ì¶œ**: ì–´ë–¤ ì¢…ë¥˜ì˜ ë¬¸ì„œë“  ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì¼ë°˜ì ì¸ ì ‘ê·¼
2. **ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜**: ì œê³µëœ ìŠ¤í‚¤ë§ˆì˜ ë…¸ë“œ íƒ€ì…ë§Œ ì‚¬ìš©
3. **ì™„ì „í•œ ì¶”ì¶œ**: ëª¨ë“  ì¤‘ìš”í•œ ê°œì²´ í¬ì°©
   - ê³ ìœ ëª…ì‚¬, ê°œë…, ìš©ì–´, ì•½ì–´ ë“±
   - ë¬¸ë§¥ìƒ ì¤‘ìš”í•œ ëª¨ë“  ê°œì²´
4. **ì¼ê´€ì„± ìœ ì§€**: ê°™ì€ ê°œì²´ëŠ” ê°™ì€ ì´ë¦„ìœ¼ë¡œ
5. **ì†ì„± ì¶”ê°€**: ê°€ëŠ¥í•œ í•œ ë§ì€ ê´€ë ¨ ì •ë³´ë¥¼ propertiesì— í¬í•¨

### íŒ¨í„´ ê¸°ë°˜ í›„ë³´ (ì°¸ê³ ìš©):
{json.dumps(pattern_candidates[:20], ensure_ascii=False, indent=2)}

{existing_names_str}

### í…ìŠ¤íŠ¸:
{content}

### ì¶œë ¥ (JSON):
{{
  "nodes": [
    {{
      "label": "ìŠ¤í‚¤ë§ˆì—_ì •ì˜ëœ_íƒ€ì…",
      "name": "ì—”í‹°í‹°_ì´ë¦„", 
      "properties": {{
        "ì¶”ê°€_ì†ì„±": "ê°’"
      }}
    }}
  ]
}}"""

    print(f"[{idx}] {filename} ì²˜ë¦¬ ì¤‘...")
    
    response = client.chat.completions.create(
        model="gpt-4.1",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )
    
    gpt_output = response.choices[0].message.content
    parsed_json = parse_json(gpt_output)
    
    # ì¶”ì¶œëœ ë…¸ë“œ ì²˜ë¦¬
    new_nodes = []
    for node in parsed_json.get('nodes', []):
        is_new, fingerprint = entity_tracker.add_entity(node, content[:500])
        if is_new:
            new_nodes.append(node)
    
    result = {
        'nodes': new_nodes,
        'relations': []  # ê´€ê³„ëŠ” ë³„ë„ ë‹¨ê³„ì—ì„œ ì¶”ì¶œ
    }
    
    # ê°œë³„ ê²°ê³¼ ì €ì¥
    result_path = os.path.join(result_dir, f'result_{idx}.json')
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=4)
    
    print(f"[{idx}] ë…¸ë“œ ì¶”ì¶œ ì™„ë£Œ - ì‹ ê·œ: {len(new_nodes)}ê°œ")
    return result_path, result

def main(purpose="ë¬¸ì„œ ë¶„ì„"):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
    result_dir = os.path.join(OUTPUT_ROOT, "result")
    chunks_dir = os.path.join(OUTPUT_ROOT, "chunked_document")
    schema_dir = os.path.join(OUTPUT_ROOT, "schema")
    os.makedirs(result_dir, exist_ok=True)
    
    # ìŠ¤í‚¤ë§ˆ ë¡œë“œ
    schema_path = os.path.join(schema_dir, "schema.json")
    if not os.path.exists(schema_path):
        print(f"âš ï¸ ìŠ¤í‚¤ë§ˆ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {schema_path}")
        print("ë¨¼ì € extract_schema.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    with open(schema_path, "r", encoding="utf-8") as f:
        schema_json = json.load(f)
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    
    system_msg = f"""ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ë¬¸ì„œì—ì„œ ì—”í‹°í‹°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
íŠ¹ì • ë„ë©”ì¸ì— í¸í–¥ë˜ì§€ ì•Šê³ , ì œê³µëœ ìŠ¤í‚¤ë§ˆì— ë”°ë¼ ì¼ê´€ì„± ìˆê²Œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""
    
    file_list = sorted([f for f in os.listdir(chunks_dir) if f.endswith('.txt')])
    n_files = len(file_list)
    
    if n_files == 0:
        print("âš ï¸ ì²˜ë¦¬í•  ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°°ì¹˜ ì²˜ë¦¬
    chunk_size = 10
    pbar = tqdm(range(0, n_files, chunk_size), desc="Processing batches", unit="batch")
    
    for start_idx in pbar:
        batch_files = file_list[start_idx:start_idx+chunk_size]
        
        with ThreadPoolExecutor(max_workers=min(chunk_size, 5)) as executor:
            futures = []
            for i, filename in enumerate(batch_files):
                idx = start_idx + i
                future = executor.submit(
                    process_file_general, idx, filename, chunks_dir, result_dir, 
                    schema_json, api_key, system_msg, purpose
                )
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ìµœì¢… ê²°ê³¼ ìƒì„±
    final_result = {
        'nodes': entity_tracker.get_all_entities(),
        'relations': []
    }
    
    # ìµœì¢… ì €ì¥
    result_path = os.path.join(result_dir, "result.json")
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=4)
    
    # í†µê³„ ì¶œë ¥
    print(f"âœ… ë…¸ë“œ ì¶”ì¶œ ì™„ë£Œ: {result_path}")
    print(f"ğŸ“Š ì´ ë…¸ë“œ: {len(final_result['nodes'])}ê°œ")
    
    # íƒ€ì…ë³„ ë¶„í¬
    from collections import Counter
    type_counter = Counter(node['label'] for node in final_result['nodes'])
    print("\nğŸ“ˆ ë…¸ë“œ íƒ€ì… ë¶„í¬:")
    for node_type, count in type_counter.most_common():
        print(f"   {node_type}: {count}ê°œ")

if __name__ == "__main__":
    main(purpose="ë¬¸ì„œ ë¶„ì„")