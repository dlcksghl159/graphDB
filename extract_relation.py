import os
import json
import openai
from util import merge_json, parse_json
from dotenv import load_dotenv
from deduplication import deduplicate
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Set
from collections import defaultdict

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
PURPOSE = os.getenv("PURPOSE", "ë¬¸ì„œ ë¶„ì„")

# í•˜ë“œì½”ë”©ëœ ê´€ê³„ íŒ¨í„´ ì œê±° - ëŒ€ì‹  ë™ì ìœ¼ë¡œ ë°œê²¬

def discover_relation_patterns(text: str, entities: List[Dict]) -> List[Dict]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ì ì¬ì  ê´€ê³„ íŒ¨í„´ ë°œê²¬"""
    patterns = []
    entity_names = {entity['name'] for entity in entities}
    
    # ì—”í‹°í‹° ìŒ ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ ë¶„ì„
    sentences = text.split('.')
    for sentence in sentences:
        found_entities = []
        for entity_name in entity_names:
            if entity_name in sentence:
                found_entities.append(entity_name)
        
        # ê°™ì€ ë¬¸ì¥ì— 2ê°œ ì´ìƒì˜ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´ ê´€ê³„ ê°€ëŠ¥ì„±
        if len(found_entities) >= 2:
            patterns.append({
                "sentence": sentence.strip(),
                "entities": found_entities,
                "potential_relation": True
            })
    
    return patterns

def infer_relation_type(sentence: str, entity1: str, entity2: str) -> Tuple[str, float]:
    """ë¬¸ì¥ì—ì„œ ë‘ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ íƒ€ì… ì¶”ë¡ """
    sentence_lower = sentence.lower()
    entity1_lower = entity1.lower()
    entity2_lower = entity2.lower()
    
    # ë™ì‚¬ ê¸°ë°˜ ê´€ê³„ ì¶”ë¡  (ì–¸ì–´ ë…ë¦½ì )
    # ê¸°ë³¸ì ì¸ ê´€ê³„ íŒ¨í„´ë§Œ ì •ì˜
    basic_patterns = {
        # ì—°ê²°/í¬í•¨ ê´€ê³„
        'contains': ['contains', 'includes', 'has', 'comprises', 'consists of', 'í¬í•¨', 'êµ¬ì„±'],
        'part_of': ['part of', 'belongs to', 'member of', 'in', 'ì†í•œ', 'ì¼ë¶€'],
        'related_to': ['related to', 'associated with', 'connected to', 'ê´€ë ¨', 'ì—°ê´€'],
        
        # ë™ì‘ ê´€ê³„
        'uses': ['uses', 'utilizes', 'employs', 'applies', 'ì‚¬ìš©', 'í™œìš©'],
        'creates': ['creates', 'produces', 'generates', 'makes', 'ìƒì„±', 'ë§Œë“¤ë‹¤'],
        'affects': ['affects', 'influences', 'impacts', 'ì˜í–¥', 'ì‘ìš©'],
        
        # ìœ„ì¹˜ ê´€ê³„
        'located_in': ['located in', 'found in', 'situated in', 'ìœ„ì¹˜', 'ìˆë‹¤'],
        
        # ì‹œê°„ ê´€ê³„
        'precedes': ['before', 'precedes', 'prior to', 'ì´ì „', 'ì•ì„œ'],
        'follows': ['after', 'follows', 'subsequent to', 'ì´í›„', 'ë”°ë¼'],
    }
    
    # íŒ¨í„´ ë§¤ì¹­
    for rel_type, patterns in basic_patterns.items():
        for pattern in patterns:
            if pattern in sentence_lower:
                # ì—”í‹°í‹° ìˆœì„œ í™•ì¸
                idx1 = sentence_lower.find(entity1_lower)
                idx2 = sentence_lower.find(entity2_lower)
                if idx1 < idx2:  # entity1ì´ ë¨¼ì € ë‚˜ì˜¤ë©´
                    return rel_type.upper(), 0.7
                else:  # ìˆœì„œê°€ ë°˜ëŒ€ë©´ ê´€ê³„ë„ ë°˜ëŒ€ì¼ ìˆ˜ ìˆìŒ
                    return f"INVERSE_{rel_type.upper()}", 0.7
    
    # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê´€ê³„
    return "RELATED_TO", 0.5

def extract_relations_general(text: str, nodes: List[Dict], schema_relations: List[Dict]) -> List[Dict]:
    """ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ ê´€ê³„ ì¶”ì¶œ"""
    extracted_relations = []
    
    # ë…¸ë“œ ì´ë¦„ê³¼ íƒ€ì… ë§¤í•‘
    node_map = {node['name']: node for node in nodes}
    
    # ìŠ¤í‚¤ë§ˆì—ì„œ ê°€ëŠ¥í•œ ê´€ê³„ íƒ€ì… ì¶”ì¶œ
    valid_relation_types = {rel['relationship'] for rel in schema_relations}
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„
    sentences = text.split('.')
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # ë¬¸ì¥ì— í¬í•¨ëœ ì—”í‹°í‹° ì°¾ê¸°
        found_entities = []
        for node in nodes:
            if node['name'] in sentence:
                found_entities.append(node)
        
        # 2ê°œ ì´ìƒì˜ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´ ê´€ê³„ ì¶”ì¶œ ì‹œë„
        if len(found_entities) >= 2:
            # ëª¨ë“  ì—”í‹°í‹° ìŒì— ëŒ€í•´ ê´€ê³„ ê²€ì‚¬
            for i in range(len(found_entities)):
                for j in range(i + 1, len(found_entities)):
                    entity1 = found_entities[i]
                    entity2 = found_entities[j]
                    
                    # ê´€ê³„ íƒ€ì… ì¶”ë¡ 
                    rel_type, confidence = infer_relation_type(sentence, entity1['name'], entity2['name'])
                    
                    # ìŠ¤í‚¤ë§ˆì— ìˆëŠ” ê´€ê³„ íƒ€ì…ê³¼ ë§¤ì¹­
                    if rel_type in valid_relation_types:
                        final_rel_type = rel_type
                    else:
                        # ê°€ì¥ ìœ ì‚¬í•œ ìŠ¤í‚¤ë§ˆ ê´€ê³„ íƒ€ì… ì°¾ê¸°
                        final_rel_type = find_closest_schema_relation(rel_type, valid_relation_types)
                    
                    extracted_relations.append({
                        "start_node": entity1['name'],
                        "relationship": final_rel_type,
                        "end_node": entity2['name'],
                        "properties": {
                            "confidence": confidence,
                            "evidence": sentence[:200]
                        }
                    })
    
    return extracted_relations

def find_closest_schema_relation(rel_type: str, valid_types: Set[str]) -> str:
    """ìŠ¤í‚¤ë§ˆì— ì •ì˜ëœ ê´€ê³„ íƒ€ì… ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°"""
    # ê°„ë‹¨í•œ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
    from difflib import get_close_matches
    
    matches = get_close_matches(rel_type, valid_types, n=1, cutoff=0.5)
    if matches:
        return matches[0]
    
    # ê¸°ë³¸ê°’
    return "RELATED_TO" if "RELATED_TO" in valid_types else list(valid_types)[0]

def process_file_general_relations(filename, chunks_dir, result_dir, system_msg, nodes, api_key, schema_relations):
    """ì¼ë°˜í™”ëœ íŒŒì¼ë³„ ê´€ê³„ ì¶”ì¶œ"""
    client = openai.OpenAI(api_key=api_key)
    i = int(filename.split('_')[-1].split('.')[0])
    
    filename_path = os.path.join(chunks_dir, filename)
    if not os.path.exists(filename_path):
        print(f"íŒŒì¼ ì—†ìŒ: {filename_path} â†’ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    with open(filename_path, "r", encoding="utf-8") as f:
        content = f.read()

    # íŒ¨í„´ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
    pattern_relations = extract_relations_general(content, nodes, schema_relations)
    
    # LLM ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
    prompt = f"""ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ëª¨ë“  ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

### ìŠ¤í‚¤ë§ˆì˜ ê´€ê³„ íƒ€ì…:
{json.dumps(schema_relations, ensure_ascii=False, indent=2)}

### ì§€ì¹¨:
1. ì œê³µëœ ë…¸ë“œë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš” (ì´ë¦„ ìˆ˜ì • ê¸ˆì§€)
2. ìŠ¤í‚¤ë§ˆì— ì •ì˜ëœ ê´€ê³„ íƒ€ì…ì„ ìš°ì„  ì‚¬ìš©í•˜ì„¸ìš”
3. ìƒˆë¡œìš´ ê´€ê³„ íƒ€ì…ì´ í•„ìš”í•˜ë©´ UPPER_SNAKE_CASEë¡œ ì‘ì„±
4. ë„ë©”ì¸ì— ê´€ê³„ì—†ì´ ì¼ë°˜ì ì¸ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
   - í¬í•¨/êµ¬ì„± ê´€ê³„ (CONTAINS, PART_OF)
   - ì—°ê´€ ê´€ê³„ (RELATED_TO, ASSOCIATED_WITH)
   - ì˜ì¡´ ê´€ê³„ (DEPENDS_ON, REQUIRES)
   - ìˆœì„œ ê´€ê³„ (PRECEDES, FOLLOWS)
   - ê¸°íƒ€ ë¬¸ë§¥ìƒ ì¤‘ìš”í•œ ê´€ê³„
5. propertiesì—ëŠ” ê´€ê³„ì˜ ê·¼ê±°, ê°•ë„, ë‚ ì§œ ë“± ì¶”ê°€ ì •ë³´ í¬í•¨

### íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ ê²°ê³¼ (ì°¸ê³ ):
{json.dumps(pattern_relations[:5], ensure_ascii=False, indent=2)}

### í…ìŠ¤íŠ¸:
{content}

### ì—”í‹°í‹°(ë…¸ë“œ):
{json.dumps(nodes, ensure_ascii=False, indent=2)}

### ì¶œë ¥ í˜•ì‹ (JSON):
{{
    "nodes": {json.dumps(nodes, ensure_ascii=False)},
    "relations": [
        {{
            "start_node": "<ì •í™•í•œì—”í‹°í‹°ì´ë¦„>",
            "relationship": "<RELATION_TYPE>",
            "end_node": "<ì •í™•í•œì—”í‹°í‹°ì´ë¦„>",
            "properties": {{
                "confidence": "high|medium|low",
                "evidence": "<ê·¼ê±° ë¬¸ì¥ ì¼ë¶€>"
            }}
        }}
    ]
}}"""

    response = client.chat.completions.create(
        model="gpt-4.1",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )
    
    gpt_output = response.choices[0].message.content
    llm_result = parse_json(gpt_output)
    
    # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
    all_relations = pattern_relations + llm_result.get('relations', [])
    
    seen_relations = set()
    unique_relations = []
    
    for rel in all_relations:
        rel_key = (rel['start_node'], rel['relationship'], rel['end_node'])
        if rel_key not in seen_relations:
            seen_relations.add(rel_key)
            unique_relations.append(rel)
    
    final_result = {
        "nodes": nodes,
        "relations": unique_relations
    }
    
    # ê°œë³„ ê²°ê³¼ ì €ì¥
    result_path = os.path.join(result_dir, f'result_enhanced_{i}.json')
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=4)
    
    print(f"[{i}] ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ - ì´ {len(unique_relations)}ê°œ")

def cross_document_relation_discovery(all_nodes: List[Dict], all_relations: List[Dict], api_key: str) -> List[Dict]:
    """ë¬¸ì„œ ì „ì²´ì—ì„œ ì¶”ê°€ ê´€ê³„ ë°œê²¬ (ì„ íƒì )"""
    client = openai.OpenAI(api_key=api_key)
    
    # ë…¸ë“œë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
    nodes_by_type = defaultdict(list)
    for node in all_nodes:
        nodes_by_type[node['label']].append(node['name'])
    
    # íƒ€ì… ê°„ ê°€ëŠ¥í•œ ê´€ê³„ ì¶”ë¡ 
    prompt = f"""ë‹¤ìŒ ì—”í‹°í‹°ë“¤ ê°„ì˜ ì¶”ê°€ì ì¸ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.

### ì—”í‹°í‹° íƒ€ì…ë³„ ëª©ë¡:
{json.dumps(dict(nodes_by_type), ensure_ascii=False, indent=2)}

### ì´ë¯¸ ë°œê²¬ëœ ê´€ê³„ íŒ¨í„´:
{json.dumps(all_relations[:10], ensure_ascii=False, indent=2)}

### ì§€ì¹¨:
1. ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ê´€ê³„ë§Œ ì¶”ë¡ í•˜ì„¸ìš”
2. ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ìëª…í•œ ê´€ê³„ëŠ” ì œì™¸í•˜ì„¸ìš”
3. ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì˜ë¯¸ìˆëŠ” ê´€ê³„ë¥¼ ì°¾ìœ¼ì„¸ìš”

### ì¶œë ¥ (JSON):
{{
    "relations": [
        {{
            "start_node": "<ì—”í‹°í‹°ëª…>",
            "relationship": "<ê´€ê³„íƒ€ì…>",
            "end_node": "<ì—”í‹°í‹°ëª…>",
            "properties": {{
                "inferred": true,
                "reasoning": "<ì¶”ë¡  ê·¼ê±°>"
            }}
        }}
    ]
}}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ì§€ì‹ ê·¸ë˜í”„ ê´€ê³„ ì¶”ë¡  ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        result = parse_json(response.choices[0].message.content)
        return result.get('relations', [])
    
    except Exception as e:
        print(f"ì¶”ê°€ ê´€ê³„ ì¶”ë¡  ì˜¤ë¥˜: {e}")
        return []

def main(purpose="ë¬¸ì„œ ë¶„ì„"):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    OUTPUT_ROOT = os.getenv("OUTPUT_ROOT", "output")
    result_dir = os.path.join(OUTPUT_ROOT, "result")
    chunks_dir = os.path.join(OUTPUT_ROOT, "chunked_document")
    schema_dir = os.path.join(OUTPUT_ROOT, "schema")
    os.makedirs(result_dir, exist_ok=True)

    # ë…¸ë“œ ì •ë³´ ë¡œë“œ
    result_path = os.path.join(result_dir, "result.json")
    if not os.path.exists(result_path):
        print(f"âš ï¸ ë…¸ë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤: {result_path}")
        print("ë¨¼ì € extract_node.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
        
    with open(result_path, "r", encoding="utf-8") as f:
        result_json = json.load(f)
        nodes = result_json["nodes"]

    # ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë“œ
    schema_path = os.path.join(schema_dir, "schema.json")
    with open(schema_path, "r", encoding="utf-8") as f:
        schema_json = json.load(f)
        schema_relations = schema_json.get("relations", [])

    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")

    system_msg = f"""ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ë¬¸ì„œì—ì„œ ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë„ë©”ì¸ì— ê´€ê³„ì—†ì´ ì˜ë¯¸ìˆëŠ” ê´€ê³„ë¥¼ ë°œê²¬í•˜ê³ , ëª…í™•í•œ ê·¼ê±°ì™€ í•¨ê»˜ ì¶”ì¶œí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""

    file_names = [f for f in os.listdir(chunks_dir) if f.endswith('.txt')]
    
    if not file_names:
        print("âš ï¸ ì²˜ë¦¬í•  ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì²­í¬ë³„ ê´€ê³„ ì¶”ì¶œ
    all_relations = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {
            executor.submit(process_file_general_relations, filename, chunks_dir, result_dir, 
                          system_msg, nodes, api_key, schema_relations): filename
            for filename in file_names
        }
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="Extracting relations"):
            try:
                future.result()
            except Exception as e:
                print(f"Error processing {futures[future]}: {e}")

    # ê²°ê³¼ ìˆ˜ì§‘
    for i in range(len(file_names)):
        chunked_result_path = os.path.join(result_dir, f"result_enhanced_{i}.json")
        if os.path.exists(chunked_result_path):
            with open(chunked_result_path, "r", encoding="utf-8") as f:
                chunked_data = json.load(f)
                all_relations.extend(chunked_data.get("relations", []))

    # ì„ íƒì : ì¶”ê°€ ê´€ê³„ ì¶”ë¡ 
    print("ğŸ”„ ì¶”ê°€ ê´€ê³„ ì¶”ë¡  ì¤‘...")
    inferred_relations = cross_document_relation_discovery(nodes, all_relations[:50], api_key)
    all_relations.extend(inferred_relations)

    # ìµœì¢… ê²°ê³¼ êµ¬ì„±
    final_result = {
        "nodes": nodes,
        "relations": all_relations
    }

    # ë…¸ë“œ ì¡´ì¬ í™•ì¸
    node_names = {node['name'] for node in nodes}
    validated_relations = []
    
    for rel in all_relations:
        if rel['start_node'] in node_names and rel['end_node'] in node_names:
            validated_relations.append(rel)
        else:
            print(f"âš ï¸ ê´€ê³„ ì œì™¸ (ë…¸ë“œ ì—†ìŒ): {rel['start_node']} -> {rel['end_node']}")
    
    final_result['relations'] = validated_relations

    # ì €ì¥
    result_path = os.path.join(result_dir, "result.json")
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=4)

    # ì¤‘ë³µ ì œê±°
    deduplicate(result_path)
    
    # í†µê³„ ì¶œë ¥
    print(f"âœ… ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ: {result_path}")
    print(f"ğŸ“Š ì´ ê´€ê³„: {len(validated_relations)}ê°œ")
    
    # ê´€ê³„ íƒ€ì…ë³„ ë¶„í¬
    from collections import Counter
    relation_types = [rel['relationship'] for rel in validated_relations]
    relation_stats = Counter(relation_types)
    
    print("\nğŸ“ˆ ê´€ê³„ íƒ€ì… ë¶„í¬ (ìƒìœ„ 10ê°œ):")
    for rel_type, count in relation_stats.most_common(10):
        print(f"   {rel_type}: {count}ê°œ")

if __name__ == "__main__":
    main(purpose="ë¬¸ì„œ ë¶„ì„")